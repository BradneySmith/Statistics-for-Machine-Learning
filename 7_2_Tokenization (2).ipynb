{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\">Statistics for Machine Learning</h1>\n",
        "<h2 align=\"center\">Tokenization</h2>\n",
        "\n",
        "&nbsp;"
      ],
      "metadata": {
        "id": "2ewX7Gaa50zh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview\n",
        "\n",
        "Natural language problems deal with textual data which computers cannot immediately understand. For this reason, words and parts of words need to be encoded using numbers. These encodings are referred to as *tokens* and can be generated in a number of different ways. The many steps throughout the tokenization pipeline are cruicial to determining the success of a language model, and so this notebook dives deep into the methods used in many popular language models toaday.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "ITuR7om0r9uD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contents\n",
        "\n",
        "Section 1 - Introduction to Tokenization\n",
        "\n",
        "Section 2 - Normalization and Pre-Tokenization\n",
        "\n",
        "Section 3 - Subword Tokenization Methods\n",
        "\n",
        "Section 4 - Tokenizers in Python Libraries\n",
        "\n",
        "Section 5 - Conclusion\n",
        "\n",
        "Section 6 - Glossary\n",
        "\n",
        "Section 7 - Further Reading"
      ],
      "metadata": {
        "id": "R8TIwUSpr_lG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependancies"
      ],
      "metadata": {
        "id": "U0ICfMvUAlMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tokenizers"
      ],
      "metadata": {
        "id": "RbFg5q4RApkJ"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align=\"center\">Section 1 - Introduction to Tokenization</h2>"
      ],
      "metadata": {
        "id": "MhGtXpLh5-EP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 - Overview of Tokenizers\n",
        "\n",
        "Natural language problems concern textual data, which cannot be immediately understood by a machine. For computers to process language, they must first convert the text into a numerical form. This process is carried out in two stages by the a component of the model called the **tokenizer**. The tokenizer first takes the text and divides it into smaller pieces, be that words, parts of words, or individual characters. These smaller pieces of text are called **tokens**. The Stanford NLP Group [1] defines tokens more rigorously as:\n",
        "\n",
        "> an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing.\n",
        "\n",
        "Once the tokenizer has divided the text into tokens, each token can be assigned a number. An example of this might be that the word 'cat' is assigned the value '15', and so every 'cat' token in the input text will be represented by the number 15.\n",
        "\n",
        "&nbsp;"
      ],
      "metadata": {
        "id": "3NGYcmq8ONl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 - Different Tokenization Methods\n",
        "\n",
        "There are several different ways to divide text into token, with the most common three being:\n",
        "\n",
        "* Word-based\n",
        "* Character-based\n",
        "* Subword-based\n",
        "\n",
        "The following cells give an overview of each of these methods, along with some pros and cons for each.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "Dx6la0GJQTIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 - Overview of Word-Based Tokenization Methods\n",
        "\n",
        "Word-based tokenization is perhaps the most simple of the three methods described above. In this method, the tokenizer will split a sentence into words by splitting on each space in the sentence (sometimes called 'whitespace-based tokenization), or by a similar set of rules (such as punctuation-based tokenization, treebank tokenization, etc) [2].\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "For example, the sentence:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "`This sentence is a great, interesting sentence!`\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "could be split on whitespace characters to give:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "`['This', 'sentence', 'is', 'a', 'great,', 'interesting', 'sentence!']`\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "or by split on both punctuation and spaces to give:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "`['This', 'sentence', 'is', 'a', 'great', ',', 'interesting', 'sentence', '!']`\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "From this simple example, it is clear that the rules used to determine the split are important, since the first split gives the potentially rare token 'sentence!', while the second split gives the two, less-rare tokens 'sentence' and '!'. Care should be taken not to remove punctuation altogether, as they can carry very specific meanings. An example of this is the apostrophe, which can distinguish between the plural and possessive form of words. For example \"book's\" refers to some property of a book, as in \"the book's spine is damaged\", and \"books\" refers to many books, as in \"the books are damaged\".\n",
        "\n",
        "Once the tokens have been generated, each can be assigned a number. The next time that a token is generated that the tokenizer has already seen, it can simply assign the number that was assigned to the earlier, indentical token. For example, if the token 'sentence' is assigned the value 1 in the sentence above, and the tokenizer is given another sentence that contains the word 'sentence', this second instance (and all subsequent instances) of the word 'sentence' will also be assigned the value of 1 [3].\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "-FBOHcuWXnWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 - Pros and Cons of Word-Based Tokenization Methods\n",
        "\n",
        "The tokens produced in the word-based method contain a high degree of information, since each token contains semantic and contextual information. However, one of the largest drawbacks with this method is that very similar words are treated as completely separate tokens. For example, the connection between 'cat' and 'cats' would be non-existent, and these would be treated as separate words. This becomes a problem in large-scale applications that contain many words, as the possible number of tokens in the model's **vocabulary** (total number of words) can grow very large. English has around 170,000 words, and so including various grammatical forms for each word can lead to what is known as the **exploding vocabulary problem**. An example of this is the TransformerXL tokenizer which uses whitespace-based splitting, this led to a vocabulary size of over 250,000 [4].\n",
        "\n",
        "One way to combat this is by enforcing a hard limit on the number of tokens the model can learn (e.g. 10,000). This would classify any word outside of the 10,000 most frequent tokens as **out-of-vocabulary** (OOV), and would assign the token value of 'UNKNOWN'. This causes performance to suffer in cases where many unknown words are present, but may be a suitable compromise if the data contains mostly common words. [3]\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Summary of Pros:**\n",
        "\n",
        "* Simple\n",
        "\n",
        "* High degree of information stored in each token\n",
        "\n",
        "* Can limit vocabulary size which works well with datasets containing mostly common words\n",
        "\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Summary of Cons:**\n",
        "\n",
        "* Separate tokens are created for similar words (e.g. 'cat' and 'cats')\n",
        "\n",
        "* Can result in very large vocabulary\n",
        "\n",
        "* Limiting vocabulary can significantly degrade performance on datasets with many uncommon words\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "CyAfy9NCYPjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 - Overview of Character-Based Tokenization Methods\n",
        "\n",
        "Character-based tokenization splits sentences on each character, including letters, numbers, and special characters such as punctuation. This greatly reduces the vocabulary size, to the point where the English language can be represented with a vocabulary size of around 256, instead of the roughly 170,000 needed with word-based approaches [5]. Even east Asian languages such as Chinese and Japanese can see a significant reduction in their vocabulary size, despite using a few thousand unqiue characters on a daily basis.\n",
        "\n",
        "In a character-based tokenizer, the following sentence:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "`This sentence is a great, interesting sentence!`\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "would be converted to:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "`['T', 'h', 'i', 's', ' ', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', ' ', 'i', 's', ' ', 'a', ' ', 'g', 'r', 'e', 'a', 't', ',', ' ,'i',\n",
        "'n', 't', 'e', 'r', 'e', 's', 't', 'i', 'n', 'g', ' ', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', '!'`]\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "a8BJkRg-ZL2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6 - Pros and Cons of Character-Based Tokenization Methods\n",
        "\n",
        "Character-based approaches result in a much smaller vocabulary size when compared to word-based methods, and also result in much fewer out-of-vocabulary tokens. This even allows misspelled words to be tokenized (albeit differently than the correct form of the word), rather than being removed immediately due to the frequency-based vocabulary limit.\n",
        "\n",
        "However there are a number of drawbacks with this approach too. Firstly, the information stored in a single token produced with a character-based method is low. This is because unlike the tokens in the word-based method, no semantic or contextual meaning is captured (particularly in the case of western and alphabet-based languages, moreso than languages with logosyllabic writing systems that tend to store much more meaning in a single character. Finally, the size of the tokenized input that can be fed into a language model is limited with this method since many numbers are used to represent the input string, much moreso than representations created using the word-based approach.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Summary of Pros:**\n",
        "\n",
        "* Smaller vocabulary size\n",
        "\n",
        "* Does not remove misspelled words\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Summary of Cons:**\n",
        "\n",
        "* Low information stored in each token, little-to-no contextual or semantic meaning\n",
        "\n",
        "* Size of input to language models is limited since the output of the tokenizer contains many more numbers than a word-based approach\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "faTIXhEEaXHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7 - Overview of Subword-Based Tokenization\n",
        "\n",
        "Subword-based tokenization aims to achieve the benefits of both word-based and character-based methods, by splitting sentences within words. This means that the resulting vocabulary size is smaller than the one found in word-based methods, but larger than the one found in character-based methods. The same is also true for the amount of information stored within each token, which is also inbetween the tokens generated by the previous two methods. The subword approach uses the follow two guides lines:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "* Frequently used words should not be split into subwords, but rather be stored as entire tokens\n",
        "\n",
        "* Infrequently used words should be split into subwords\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Splitting only the infrequently used words gives a chance the conjugations, plural forms etc are decomposed into their constiutent parts and the relationship between tokens is preserved. For example 'cat' might be a very common word in the dataset, but 'cats' might be less common. For this reason, 'cats' would be split into 'cat' and 's', where 'cat' is now assigned the same value as every other 'cat' token, and 's' is assigned a different value, which can encode the meaning of plurality. Another example would be the word 'tokenization', which can be split into the root word 'token' and the suffix 'ization'. This method can therefore preserve syntactic and semantic similarity. [6] For these reasons, subword-based tokenizers are very commonly used in NLP models today.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "qcKt34SyePPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align=\"center\">Section 2 - Normalization and Pre-Tokenization</h2>\n",
        "\n"
      ],
      "metadata": {
        "id": "HmAsX58msaTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 - Overview of the Tokenization Pipeline\n",
        "\n",
        "The tokenization process requires some pre-processing and post-processing steps, that in all comprise the **tokenization pipeline**. This describes the entires series of actions that are take to convert raw text into tokens. The steps of this pipeline are:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "* Normalization\n",
        "\n",
        "* Pre-tokenization\n",
        "\n",
        "* Model\n",
        "\n",
        "* Post-processing\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "where the tokenization method (be that subword-based, character-based etc) taking place in the model step. [7] This section will cover each of these steps for a tokenizer that uses a subword-based tokenization approach.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**IMPORTANT NOTE:** all the steps of the tokenization pipeline are handled for the user automaticallly when using a tokenizer from libraries such as Hugging Face. The entire pipeline is performed by a single entity called the Tokenizer. The cells in this section dive into the inner workings of the code the most users do not need to handle manually when working with NLP tasks.\n",
        "\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "0_rmfFZ6h4ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 - Normalization Methods\n",
        "\n",
        "**Normalization** is the process of *cleaning up* the text before it is split into tokens. This includes converting each character to lowercase, removing accents from characters (e.g. 'é' becomes 'e'), removing unnecessary whitespace, and so on. For example, the string `ThÍs is  áN ExaMPlé     sÉnteNCE` becomes `this is an example sentence` after normalization. Different normalizers will perform different steps, which can be useful depending on the use-case. For example, in some situations the casing or accents might need to be preserved. Depending on the normalizer chosen, different effects can be achieved at this stage.\n",
        "\n",
        "The Hugging Face `tokenizers.normalizers` package contains several basic normalizers that are used by different tokenizers are part of larger models. The base normalizer class can be imported directly however to investigate how they work. Below shows the NFC unicode, Lowercase, and BERT normalizers. These show the following effects on the example sentence:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "* **NFC:** Does not convert casing or remove accents\n",
        "* **Lower:** Converts casing but does not remove accents\n",
        "* **BERT:** Converts casing and removes accents\n",
        "\n",
        "&nbsp;"
      ],
      "metadata": {
        "id": "GbhH1mlT9lhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.normalizers import NFC, Lowercase, BertNormalizer\n",
        "\n",
        "# Text to normalize\n",
        "example_sentence = 'ThÍs is  áN ExaMPlé     sÉnteNCE'\n",
        "\n",
        "# Instantiate normalizer objects\n",
        "NFCNorm = NFC()\n",
        "LowercaseNorm = Lowercase()\n",
        "BertNorm = BertNormalizer()\n",
        "\n",
        "# Normalize the text\n",
        "print(f'NFC:   {NFCNorm.normalize_str(example_sentence)}')\n",
        "print(f'Lower: {LowercaseNorm.normalize_str(example_sentence)}')\n",
        "print(f'BERT:  {BertNorm.normalize_str(example_sentence)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJmRZzqaHaz-",
        "outputId": "98c1343b-58d1-4c61-dd6d-636656a6add0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NFC:   ThÍs is  áN ExaMPlé     sÉnteNCE\n",
            "Lower: thís is  án examplé     séntence\n",
            "BERT:  this is  an example     sentence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "&nbsp;\n",
        "\n",
        "The normalizers above are used in tokenizer models which can be imported from the Hugging Face `transformers` library. The cell below shows that the normalizers can be accessed using dot notation via `Tokenizer.backend_tokenizer.normalizer`. Some comparisons are shown between the tokenizsers to highlight the different normalization methods that are used. Note that in these examples, only the FNet normalizer removes unncessary whitespace.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "Ym3WMKs0MX97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import FNetTokenizerFast, CamembertTokenizerFast, BertTokenizerFast\n",
        "\n",
        "# Text to normalize\n",
        "example_sentence = 'ThÍs is  áN ExaMPlé     sÉnteNCE'\n",
        "\n",
        "# Instatiate tokenizers\n",
        "FNetTokenizer = FNetTokenizerFast.from_pretrained('google/fnet-base')\n",
        "CamembertTokenizer = CamembertTokenizerFast.from_pretrained('camembert-base')\n",
        "BertTokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Normalize the text\n",
        "print(f'FNet Output:      {FNetTokenizer.backend_tokenizer.normalizer.normalize_str(example_sentence)}')\n",
        "print(f'CamemBERT Output: {CamembertTokenizer.backend_tokenizer.normalizer.normalize_str(example_sentence)}')\n",
        "print(f'BERT Output:      {BertTokenizer.backend_tokenizer.normalizer.normalize_str(example_sentence)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2THnB6kLAEzE",
        "outputId": "49c72fc1-d99e-462a-da48-907d6b1bafbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FNet Output:      ThÍs is áN ExaMPlé sÉnteNCE\n",
            "CamemBERT Output: ThÍs is  áN ExaMPlé     sÉnteNCE\n",
            "BERT Output:      this is  an example     sentence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 - Pre-Tokenization Methods\n",
        "\n",
        "The **pre-tokenization** step is the first splitting of the raw text in the tokenization pipeline. The split is performed to give an upper bound to what the final tokens could be at the end of the pipeline. That is, a sentence can be split into words in the pre-tokenization step, then in the model step some of these words may be split further according to the tokenization method (e.g. subword-based).\n",
        "\n",
        "Just like with normalization, there are several ways that this step can be performed. For example, a sentence can be split based on every space, every space and some punctuation, or every space and every punctuation.\n",
        "\n",
        "The cell below shows a comparison between the basic `Whitespacesplit` pre-tokenizer, and slightly more complex `BertPreTokenizer` from the Hugging Face `tokenizers.pre_tokenizers` package. The output of the whitespace pre-tokenizer leaves the punctuation in-tact, and still attached to the neighbouring words. For example `includes:` is treated as a single word in this case. Whereas the BERT pre-tokenizer treats punctuation as individual words [8].\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "nWqnWe9ANT2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.pre_tokenizers import WhitespaceSplit, BertPreTokenizer\n",
        "\n",
        "# Text to pre-tokenize - written in lowercase with no unnecessary whitespace to simulate normalization step\n",
        "example_sentence = \"this sentence's content includes: characters, spaces, and punctuation.\"\n",
        "\n",
        "# Define helper function to display pre-tokenized output\n",
        "def print_pretokenized_str(pre_tokens):\n",
        "    for pre_token in pre_tokens:\n",
        "        print(f'\"{pre_token[0]}\", ', end='')\n",
        "\n",
        "# Instantiate pre-tokenizers\n",
        "wss = WhitespaceSplit()\n",
        "bpt = BertPreTokenizer()\n",
        "\n",
        "# Pre-tokenize the text\n",
        "print('Whitespace Pre-Tokenizer:')\n",
        "print_pretokenized_str(wss.pre_tokenize_str(example_sentence))\n",
        "\n",
        "print('\\n\\nBERT Pre-Tokenizer:')\n",
        "print_pretokenized_str(bpt.pre_tokenize_str(example_sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZkMWhcBjRTD",
        "outputId": "bf8163e7-bdc3-4659-e2da-6448cb02034f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Pre-Tokenizer:\n",
            "\"this\", \"sentence's\", \"content\", \"includes:\", \"characters,\", \"spaces,\", \"and\", \"punctuation.\", \n",
            "\n",
            "BERT Pre-Tokenizer:\n",
            "\"this\", \"sentence\", \"'\", \"s\", \"content\", \"includes\", \":\", \"characters\", \",\", \"spaces\", \",\", \"and\", \"punctuation\", \".\", "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "&nbsp;\n",
        "\n",
        "Just as with the normalization methods, you can call the pre-tokenization methods directly from common tokenizers such as the GPT-2 and ALBERT (A Lite BERT) tokenizers. These take a slightly different approach to the standard BERT pre-tokenizer shown above, in that space characters are not removed when splitting the tokens. Instead, they are replaced with special characters that represent where the space was. This has the advantage in that the space characters can be ignored when processing further, but the original sentence can be retrieved if required. The GPT-2 model uses the `Ġ` which features a capital 'G' with a dot above. The ALBERT models uses a special underscore character.\n",
        "\n",
        "&nbsp;"
      ],
      "metadata": {
        "id": "vDy0lC50qLv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Text to pre-tokenize - written in lowercase with no unnecessary whitespace to simulate normalization step\n",
        "example_sentence = \"this sentence's content includes: characters, spaces, and punctuation.\"\n",
        "\n",
        "# Instatiate the pre-tokenizers\n",
        "GPT2_PreTokenizer = AutoTokenizer.from_pretrained('gpt2').backend_tokenizer.pre_tokenizer\n",
        "Albert_PreTokenizer = AutoTokenizer.from_pretrained('albert-base-v1').backend_tokenizer.pre_tokenizer\n",
        "\n",
        "# Pre-tokenize the text\n",
        "print('GPT-2 Pre-Tokenizer:')\n",
        "print_pretokenized_str(GPT2_PreTokenizer.pre_tokenize_str(example_sentence))\n",
        "print('\\n\\nALBERT Pre-Tokenizer:')\n",
        "print_pretokenized_str(Albert_PreTokenizer.pre_tokenize_str(example_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9KHyQ3cpr4C",
        "outputId": "1e0f6602-1138-4562-aa0b-a852e1a890da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-2 Pre-Tokenizer:\n",
            "\"this\", \"Ġsentence\", \"'s\", \"Ġcontent\", \"Ġincludes\", \":\", \"Ġcharacters\", \",\", \"Ġspaces\", \",\", \"Ġand\", \"Ġpunctuation\", \".\", \n",
            "\n",
            "Albert Pre-Tokenizer:\n",
            "\"▁this\", \"▁sentence's\", \"▁content\", \"▁includes:\", \"▁characters,\", \"▁spaces,\", \"▁and\", \"▁punctuation.\", "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "&nbsp;\n",
        "\n",
        "The cells above show the output of pre-tokenization is a compact format that nicely fits on the screen and removes some of the additional information generated. Below shows the results of a BERT pre-tokenization step on the same example sentence without any modifications. The object returned is a Python list containing tuples. Each tuple corresponds to a pre-token, where the first element is the pre-token string, and the second element is a tuple containing the index for the start and end of the string in the original input text. Note that the starting index of the string is inclusive, and the ending index is exclusive.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "lvUONlVKsCaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.pre_tokenizers import WhitespaceSplit, BertPreTokenizer\n",
        "\n",
        "# Text to pre-tokenize - written in lowercase with no unnecessary whitespace to simulate normalization step\n",
        "example_sentence = \"this sentence's content includes: characters, spaces, and punctuation.\"\n",
        "\n",
        "# Instantiate pre-tokenizer\n",
        "bpt = BertPreTokenizer()\n",
        "\n",
        "# Pre-tokenize the text\n",
        "bpt.pre_tokenize_str(example_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-TmHxqLsVH4",
        "outputId": "b3a400ce-f9c1-47c3-ac42-554448ae0de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('this', (0, 4)),\n",
              " ('sentence', (5, 13)),\n",
              " (\"'\", (13, 14)),\n",
              " ('s', (14, 15)),\n",
              " ('content', (16, 23)),\n",
              " ('includes', (24, 32)),\n",
              " (':', (32, 33)),\n",
              " ('characters', (34, 44)),\n",
              " (',', (44, 45)),\n",
              " ('spaces', (46, 52)),\n",
              " (',', (52, 53)),\n",
              " ('and', (54, 57)),\n",
              " ('punctuation', (58, 69)),\n",
              " ('.', (69, 70))]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align=\"center\">Section 3 - Subword Tokenization Methods</h2>\n",
        "\n"
      ],
      "metadata": {
        "id": "IGEFQnFsHZmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 - Subword Tokenization Methods\n",
        "\n",
        "The model step of the tokenization pipeline is where the tokenization method comes into play. As described earlier, the options here are: word-based, character-based, and subword-based. Subword-based are generally favoured, since these methods were designed to overcome the limitations of the word and character-based approaches.\n",
        "\n",
        "For transformer models, there are three tokenizer methods that are commonly used to implement subword-based tokenization. These include:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "* Byte Pair Encoding (BPE)\n",
        "\n",
        "* WordPiece\n",
        "\n",
        "* Unigram\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Each of these use slightly different techniques to split the less frequent words into smaller tokens, which are laid out in the next few cells. In addition, an implementation of these algorithms written in vanilla Python is also shown. This should help give a solid intuition for how these methods divide text into tokens, and the differences in their implementations.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "kb62ki1fOLKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 - Byte Pair Encoding (BPE) Tokenization\n",
        "\n",
        "The BPE algorithm is a commonly-used tokenizer that is found in many transformer models such as Open AI's GPT and GPT-2 models, BART, and many others [9-10]. It was originally designed as a text compression algorithm, but has been found to work very well in tokenization tasks for language models. The BPE algorithm aims to decompose a string of text into subword units that appear frequently in a reference corpus (the text used to train the tokenization model) [11]. The BPE model is trained as follows:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 1) Construct the Corpus**\n",
        "\n",
        "The input text is given to the normalization and pre-tokenization models to create clean words. The words are then given to the BPE model, which determines the frequency of each word, and stores this number alongside the word in a list called the **corpus**.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 2) Construct the Vocabulary**\n",
        "\n",
        "The words from the corpus are then broken down individual characters and are added to an empty list called the vocabulary. The algorithm will iteratively add to this vocabulary every time it determines which character pairs can be merged together.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 3) Find the Frequency of Character Pairs**\n",
        "\n",
        "The frequency of character pairs is then recorded for each word in the corpus. For example, the words 'cats' will have the character pairs 'ca', 'at', and 'ts'. All the words are examined in this way, and contribute to a global frequency counter. So any instance of 'ca' found in any of the tokens will increase the frequency counter for the 'ca' pair.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 4) Create a Merging Rule**\n",
        "\n",
        "When the frequency for each character pair is known, the most frequent character pair is added to the vocabulary. The vocabulary now consists of every individual letter in the tokens, plus the most frequent character pair. This also gives a merging rule that the model can use. For example, if the model knows that 'ca' is the most frequent character pair, it has learned that all adjacent instances of 'c' and 'a' in the corpus can be merged to give 'ca'. This can now be treated as a single character 'ca' for the remainder of the steps.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 5) Repeat Steps 3 and 4**\n",
        "\n",
        "Steps 3 and are then repeated, finding more merging rules, and adding more character pairs to the vocabulary. This process continues until the vocabulary size reaches a target size specified at the beginning of the training.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Now that the BPE algorithm has been trained (i.e. now that all have the merging rules have been found), the model can be used to tokenize any text by first splitting each of the words on every character, and then merging according to the merge rules.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "rs5-JG_jthF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 - Implementation of BPE in Python\n",
        "\n",
        "Below shows a vanilla Python implementation of the BPE algorithm, following the steps outlined above. The next cell will show the code in action using a toy dataset.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "eDMZNTmEFZUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TargetVocabularySizeError(Exception):\n",
        "    def __init__(self, message):\n",
        "        super().__init__(message)\n",
        "\n",
        "class BPE:\n",
        "    '''An implementation of the Byte Pair Encoding tokenizer.'''\n",
        "\n",
        "    def __init__(self, words, target_vocab_size):\n",
        "        self.words = words\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "        self.corpus = self.initialise_corpus(self.words)\n",
        "        self.corpus_history = [self.corpus]\n",
        "        self.vocabulary = list(set(''.join(words)))\n",
        "        self.vocabulary_size = len(self.vocabulary)\n",
        "        self.merge_rules = []\n",
        "\n",
        "        # Iteratively add vocabulary until the target vocabulary size is reached\n",
        "        if len(self.vocabulary) > self.target_vocab_size:\n",
        "            raise TargetVocabularySizeError(f'Error: Target vocabulary size \\\n",
        "            must be greater than the initial vocabulary size \\\n",
        "            ({len(self.vocabulary)})')\n",
        "\n",
        "        else:\n",
        "            while len(self.vocabulary) < self.target_vocab_size:\n",
        "                try:\n",
        "                    self.create_merge_rule(self.corpus)\n",
        "                    self.corpus = self.merge(self.corpus)\n",
        "                    self.corpus_history.append(self.corpus)\n",
        "\n",
        "                # If no further merging is possible\n",
        "                except ValueError:\n",
        "                    print('Exiting: No further merging is possible')\n",
        "                    break\n",
        "\n",
        "\n",
        "    def calculate_frequency(self, words):\n",
        "        ''' Calculate the frequency for each word in a list of words.\n",
        "\n",
        "            Take in. list of words stored as strings and return a list of tuples\n",
        "            where each tuple contains a string from the words list, and an\n",
        "            integer representing its frequency count in the list.\n",
        "\n",
        "            Args:\n",
        "                words (list):  A list of words (strings) in any order.\n",
        "\n",
        "            Returns:\n",
        "                corpus (list[tuple(str, int)]):\n",
        "                               A list of tuples where the first element is a\n",
        "                               string of a word in the words list, and the\n",
        "                               second element is an integer representing the\n",
        "                               frequency of the word in the list.\n",
        "        '''\n",
        "        freq_dict = dict()\n",
        "\n",
        "        for word in words:\n",
        "            if word not in freq_dict:\n",
        "                freq_dict[word] = 1\n",
        "            else:\n",
        "                freq_dict[word] += 1\n",
        "\n",
        "        corpus = [(word, freq_dict[word]) for word in freq_dict.keys()]\n",
        "\n",
        "        return corpus\n",
        "\n",
        "\n",
        "    def initialise_corpus(self, words):\n",
        "        ''' Split each word into characters and count the word frequency.\n",
        "\n",
        "            Split each word in the input word list on every character. For each\n",
        "            word, store the split word in a list as the first element inside a\n",
        "            tuple. Store the frequency count of the word as an integer as the\n",
        "            second element of the tuple. Create a tuple for every word in this\n",
        "            fashion and store the tuples in a list called 'corpus', then return\n",
        "            then corpus list.\n",
        "\n",
        "            Args:\n",
        "                None\n",
        "\n",
        "            Returns:\n",
        "                corpus (list[tuple(list, int)]):\n",
        "                               A list of tuples where the first element is a\n",
        "                               list of a word in the words list (where the\n",
        "                               elements are the individual characters of the\n",
        "                               word), and the second element is an integer\n",
        "                               representing the frequency of the word in the\n",
        "                               list.\n",
        "        '''\n",
        "        corpus = self.calculate_frequency(words)\n",
        "        corpus = [([*word], freq) for (word, freq) in corpus]\n",
        "        return corpus\n",
        "\n",
        "\n",
        "    def find_pair_frequencies(self, corpus):\n",
        "        ''' Find the frequency of each character pair in the corpus.\n",
        "\n",
        "            Loops through the corpus and calculate the frequency of each pair\n",
        "            of adjacent characters across every word. Return a dictionary of\n",
        "            each character pair as the keys and the corresponding frequency as\n",
        "            the values.\n",
        "\n",
        "            Args:\n",
        "                corpus (list[tuple(list, int)]):\n",
        "                               A list of tuples where the first element is a\n",
        "                               list of a word in the words list (where the\n",
        "                               elements are the individual characters (or\n",
        "                               subwords in later iterations) of the\n",
        "                               word), and the second element is an integer\n",
        "                               representing the frequency of the word in the\n",
        "                               list.\n",
        "\n",
        "            Returns:\n",
        "                pair_freq_dict (dict): A dictionary where the keys are the\n",
        "                                       character pairs from the input corpus and\n",
        "                                       the values are an integer representing\n",
        "                                       the frequency of the pair in the corpus.\n",
        "        '''\n",
        "        pair_freq_dict = dict()\n",
        "\n",
        "        for word, word_freq in corpus:\n",
        "            for idx in range(len(word)-1):\n",
        "\n",
        "                char_pair = f'{word[idx]},{word[idx+1]}'\n",
        "\n",
        "                if char_pair not in pair_freq_dict:\n",
        "                    pair_freq_dict[char_pair] = word_freq\n",
        "                else:\n",
        "                    pair_freq_dict[char_pair] += word_freq\n",
        "\n",
        "        return pair_freq_dict\n",
        "\n",
        "\n",
        "    def create_merge_rule(self, corpus):\n",
        "        ''' Create a merge rule and add it to the self.merge_rules list.\n",
        "\n",
        "            Args:\n",
        "                corpus (list[tuple(list, int)]):\n",
        "                               A list of tuples where the first element is a\n",
        "                               list of a word in the words list (where the\n",
        "                               elements are the individual characters (or\n",
        "                               subwords in later iterations) of the\n",
        "                               word), and the second element is an integer\n",
        "                               representing the frequency of the word in the\n",
        "                               list.\n",
        "\n",
        "            Returns:\n",
        "                None\n",
        "        '''\n",
        "        pair_frequencies = self.find_pair_frequencies(corpus)\n",
        "        most_frequent_pair = max(pair_frequencies, key=pair_frequencies.get)\n",
        "        self.merge_rules.append(most_frequent_pair.split(','))\n",
        "        self.vocabulary.append(most_frequent_pair)\n",
        "\n",
        "\n",
        "    def merge(self, corpus):\n",
        "        ''' Loop through the corpus and perform the latest merge rule.\n",
        "\n",
        "            Args:\n",
        "                corpus (list[tuple(list, int)]):\n",
        "                            A list of tuples where the first element is a\n",
        "                            list of a word in the words list (where the\n",
        "                            elements are the individual characters (or\n",
        "                            subwords in later iterations) of the\n",
        "                            word), and the second element is an integer\n",
        "                            representing the frequency of the word in the\n",
        "                            list.\n",
        "\n",
        "            Returns:\n",
        "                new_corpus (list[tuple(list, int)]):\n",
        "                            A modified version of the input argument where the\n",
        "                            most recent merge rule has been applied to merge\n",
        "                            the most frequent adjacent characters.\n",
        "        '''\n",
        "        merge_rule = self.merge_rules[-1]\n",
        "        new_corpus = []\n",
        "\n",
        "        for word, word_freq in corpus:\n",
        "            new_word = []\n",
        "            idx = 0\n",
        "\n",
        "            while idx < len(word):\n",
        "                # If a merge pattern has been found\n",
        "                if (len(word) != 1) and (word[idx] == merge_rule[0]) and\\\n",
        "                (word[idx+1] == merge_rule[1]):\n",
        "\n",
        "                    new_word.append(word[idx]+word[idx+1])\n",
        "                    idx += 2\n",
        "                # If a merge patten has not been found\n",
        "                else:\n",
        "                    new_word.append(word[idx])\n",
        "                    idx += 1\n",
        "\n",
        "            new_corpus.append((new_word, word_freq))\n",
        "\n",
        "        return new_corpus"
      ],
      "metadata": {
        "id": "D1RqTPzwuoit"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 - Using the BPE Algorithm with a Toy Dataset\n",
        "\n",
        "The BPE algorithm is used below with a toy dataset that contains some words about cats. The goal of the tokenizer is to determine the most useful, meaningful subunits of the words in the dataset to be used as tokens. From inspection, it is clear that units such as 'cat', 'eat' and 'ing' would be useful subunits.\n",
        "\n",
        "Running the tokenizer with a target vocabulary size of 21 (which only requires 5 merges) is enough for the tokenizer to capture all the desired subunits mentioned above. With a larger dataset, the target vocabulary would be much higher, but this shows how power the BPE tokenizer can be.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "JL6-Q9FoFpgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate and use tokenizer\n",
        "words = ['cat', 'cat', 'cat', 'cat', 'cat',\n",
        "         'cats', 'cats',\n",
        "         'eat', 'eat', 'eat', 'eat', 'eat', 'eat', 'eat', 'eat', 'eat', 'eat',\n",
        "         'eating', 'eating', 'eating',\n",
        "         'running', 'running',\n",
        "         'jumping',\n",
        "         'food', 'food', 'food', 'food', 'food', 'food']\n",
        "\n",
        "# Instantiate the tokenizer\n",
        "bpe = BPE(words, 21)\n",
        "\n",
        "# Print out the corpus at each stage of the process, and the merge rule used\n",
        "print(f'INITIAL CORPUS:\\n{bpe.corpus_history[0]}\\n')\n",
        "for rule, corpus in list(zip(bpe.merge_rules, bpe.corpus_history[1:])):\n",
        "    print(f'NEW MERGE RULE: Combine \"{rule[0]}\" and \"{rule[1]}\"')\n",
        "    print(corpus, end='\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6QKvD3HCjpS",
        "outputId": "14070846-3ec8-458b-e563-6dee92f02da3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INITIAL CORPUS:\n",
            "[(['c', 'a', 't'], 5), (['c', 'a', 't', 's'], 2), (['e', 'a', 't'], 10), (['e', 'a', 't', 'i', 'n', 'g'], 3), (['r', 'u', 'n', 'n', 'i', 'n', 'g'], 2), (['j', 'u', 'm', 'p', 'i', 'n', 'g'], 1), (['f', 'o', 'o', 'd'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"a\" and \"t\"\n",
            "[(['c', 'at'], 5), (['c', 'at', 's'], 2), (['e', 'at'], 10), (['e', 'at', 'i', 'n', 'g'], 3), (['r', 'u', 'n', 'n', 'i', 'n', 'g'], 2), (['j', 'u', 'm', 'p', 'i', 'n', 'g'], 1), (['f', 'o', 'o', 'd'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"e\" and \"at\"\n",
            "[(['c', 'at'], 5), (['c', 'at', 's'], 2), (['eat'], 10), (['eat', 'i', 'n', 'g'], 3), (['r', 'u', 'n', 'n', 'i', 'n', 'g'], 2), (['j', 'u', 'm', 'p', 'i', 'n', 'g'], 1), (['f', 'o', 'o', 'd'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"c\" and \"at\"\n",
            "[(['cat'], 5), (['cat', 's'], 2), (['eat'], 10), (['eat', 'i', 'n', 'g'], 3), (['r', 'u', 'n', 'n', 'i', 'n', 'g'], 2), (['j', 'u', 'm', 'p', 'i', 'n', 'g'], 1), (['f', 'o', 'o', 'd'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"i\" and \"n\"\n",
            "[(['cat'], 5), (['cat', 's'], 2), (['eat'], 10), (['eat', 'in', 'g'], 3), (['r', 'u', 'n', 'n', 'in', 'g'], 2), (['j', 'u', 'm', 'p', 'in', 'g'], 1), (['f', 'o', 'o', 'd'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"in\" and \"g\"\n",
            "[(['cat'], 5), (['cat', 's'], 2), (['eat'], 10), (['eat', 'ing'], 3), (['r', 'u', 'n', 'n', 'ing'], 2), (['j', 'u', 'm', 'p', 'ing'], 1), (['f', 'o', 'o', 'd'], 6)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 - Issues with BPE Tokenizers\n",
        "\n",
        "BPE tokenizers can only recognise character that have appeared in the training data. For example, in the implementation above, the training data only contained the characters needed to talk about cats, which happened to not require a 'z'. Therefore, that version of the tokenizer does not the character 'z' in its vocabulary and so would convert that character to an unknown token if the model was used to tokenize real data (actually, error handling was not added to instruct the model to create unknown tokens and so it would crash, but for productionised models this is the case).\n",
        "\n",
        "The BPE tokenizers used in GPT-2 and RoBERTa do not have this issue due to a trick within the code. Instead of analysing the training data based on the Unicode characters, they instead analyse the character's bytes. This allows a small base vocabulary to be able to tokenize all characters the model might see.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "XJRqnK6mMnt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 - WordPiece Tokenization\n",
        "\n",
        "WordPiece is a tokenizer method developed by Google for their seminal BERT model, and has been used in derivative models such as DistilBERT and MobileBERT. Other models ha\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "KQrO3jWrujPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WordPiece Tokenizer\n"
      ],
      "metadata": {
        "id": "Nux2KgsBusfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 - Unigram Tokenization\n",
        "\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "oG6DHesDtmdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unigram Tokenizer\n"
      ],
      "metadata": {
        "id": "4jsotYkQuu_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align=\"center\">Section 4 - Tokenizers in Python Libraries</h2>"
      ],
      "metadata": {
        "id": "zhj1eD6ktUme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets tokenizers segeval -q"
      ],
      "metadata": {
        "id": "v7eBmbtiAIuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HuggingFace libraries\n",
        "import datasets\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "# Other libraries\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "M-WFGZee_-ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 - The CoNLL2003 Dataset\n",
        "\n",
        "The CoNNL2003 dataset is one of the most commonly used datasets for NER tasks. It was first introduced in the paper *Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition* during the 2003 Conference on Natural Language Learning (CoNLL) [1]. The aim of the task was to improve the state NER, and develop a language-independent model. As such, the dataset included data for both English and German for researchers to use in their model building.\n",
        "\n",
        "This notebook will focus solely on the English data, which was produced by taking news artilcles from the Reuters corpus. This consisted of stories between August 1996 and August 1997. The training set was constructed using 10 days worth of data from late August, and the test data was taken from December of the same year. Preprocessed raw data is also included, which was taken from September 1996.\n",
        "\n",
        "In the Hugging Face `datasets` library, this data is stored in a `DatasetDict`, which contains: train, validation, and test data, with a split of 14041 : 3250 : 3453."
      ],
      "metadata": {
        "id": "hNLsq9LR6SZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003 = datasets.load_dataset('conll2003')\n",
        "conll2003"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7b6s5lC0r4r",
        "outputId": "92d3d81f-c790-421a-a3af-ff45632d6b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below prints out the first element of the training data, which is stored in a Python dictionary. The element contains a single sentence from the Reuters articles, along with some additional information about the sentence, stored as key-value pairs. The first key, `id`, stores the id of the element, which in this case is 0. The next key, `tokens`, stores a list of all the tokens in the sentence. In this case, a token has been taken to be a word or some punctuation, such as the '.' at the end of the sentence."
      ],
      "metadata": {
        "id": "y6OLtVOAAyxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbTFJyd1AqEG",
        "outputId": "63b9f99c-5ba0-4467-fcf9-5f566ec5fa2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
              " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
              " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next key is `pos_tags`, assigns a **part-of-speech** (POS) tag to each of the tokens. The POS tag can be one of the 47 shown below, and these describe if the word is a noun, adverb, etc."
      ],
      "metadata": {
        "id": "u1pkPSaRHAiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003['train'].features['pos_tags']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFMNzr5nHBs8",
        "outputId": "10c659ca-d378-4d4a-a943-61cfacf8363c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `chunk_tags` are used to show whether tokens belong to a phrase within the sentence, and which phrase they belong too. The cell below shows the possible values that these tags can take, whith the prefixes B, I, and O, being used to show whether the token is at the beginning of a phrase, inside a phrase, or not in a phrase, respectively."
      ],
      "metadata": {
        "id": "nwLU0P4jH_BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003['train'].features['chunk_tags']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z__ltDr3Hlyu",
        "outputId": "67855d57-8aaa-4a1f-cb6c-93c133a9095b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the `ner_tags` value shows which entity type has been assigned to each of the tokens. A `0` represents no entity type has been assigned, and the other numbers represent the assigned entity type according to the list below. Note that the CoNLL2003 dataset only includes four entity types: PERSON, ORGANIZATION, LOCATION, and MISC (which represents no entity assigned)."
      ],
      "metadata": {
        "id": "vDClsSnTIihT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003['train'].features['ner_tags']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLm2L10-I5T9",
        "outputId": "1bb8b872-56bd-4442-f4af-f032ebf42e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 - Initialise the Tokenizer"
      ],
      "metadata": {
        "id": "EYMLD-uqLlQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "element_0 = conll2003['train'][0]\n",
        "tokenized_input = tokenizer(element_0['tokens'], is_split_into_words=True)\n",
        "tokenized_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erIkx0wSLtUi",
        "outputId": "a465f35e-b014-4bc8-b8c2-e4f5a41ba083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf0oB4U1M-PJ",
        "outputId": "c9feb3cc-d78c-45d4-ce45-40d3ca894831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'eu',\n",
              " 'rejects',\n",
              " 'german',\n",
              " 'call',\n",
              " 'to',\n",
              " 'boycott',\n",
              " 'british',\n",
              " 'lamb',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "[x] - [CoNLL2003 Paper](https://arxiv.org/pdf/cs/0306050v1.pdf)\n",
        "\n",
        "[1] - Token Definition [Stanford NLP Group](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html#:~:text=A%20token%20is%20an%20instance,useful%20semantic%20unit%20for%20processing.)\n",
        "\n",
        "[2] Word Tokenizers - [Towards Data Science](https://towardsdatascience.com/top-5-word-tokenizers-that-every-nlp-data-scientist-should-know-45cc31f8e8b9#:~:text=Tokenization%20is%20the%20process%20of,I%E2%80%9D%20and%20%E2%80%9Cwon%E2%80%9D.)\n",
        "\n",
        "[3] Tokenizers - [Hugging Face](https://huggingface.co/docs/transformers/tokenizer_summary)\n",
        "\n",
        "[4] TransformerXL Paper - [ArXiv](https://arxiv.org/abs/1901.02860)\n",
        "\n",
        "[5] Word-Based, Subword, and Character-Based Tokenizers - [Towards Data Science](https://towardsdatascience.com/word-subword-and-character-based-tokenization-know-the-difference-ea0976b64e17)\n",
        "\n",
        "[6] A Comprehensive Guide to Subword Tokenizers - [Towards Data Science](https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c)\n",
        "\n",
        "[7] The Tokenization Pipeline - [Hugging Face](https://huggingface.co/docs/tokenizers/pipeline)\n",
        "\n",
        "[8] Pre-tokenizers - [Hugging Face](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)\n",
        "\n",
        "[9] Language Models are Unsupervised Multitask Learners - [OpenAI](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)\n",
        "\n",
        "[10] BART Model for Text Autocompletion in NLP - [Geeks for Geeks](https://www.geeksforgeeks.org/bart-model-for-text-auto-completion-in-nlp/)\n",
        "\n",
        "[11] Byte Pair Encoding - [Hugging Face](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt#byte-pair-encoding-tokenization)"
      ],
      "metadata": {
        "id": "eFHhi50l9St6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LgIcOALJAr7e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}