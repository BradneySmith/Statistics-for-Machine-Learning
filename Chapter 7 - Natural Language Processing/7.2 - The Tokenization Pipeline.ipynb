{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\">Statistics for Machine Learning</h1>\n",
        "<h2 align=\"center\">The Tokenization Pipeline</h2>\n",
        "\n",
        "&nbsp;"
      ],
      "metadata": {
        "id": "2ewX7Gaa50zh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview\n",
        "\n",
        "Natural language problems deal with textual data which computers cannot immediately understand. For this reason, words (or parts of words) need to be encoded using numbers. These encodings are referred to as *tokens* and can be generated in a number of different ways. The many steps throughout the tokenization pipeline are cruicial to determining the success of a language model, and so this notebook dives deep into the tokenizer methods used in many popular language models toaday. To demonstrate how the theory behind these models works in practice, two different tokenizers have been written from scratch in vanilla Python. These are for the Byte Pair Encoder and WordPiece methods respectively. The notebook finishes by covering how to import and use pre-trained models from the Hugging Face `tokenizers` library, as well as how to train some pre-made models.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "ITuR7om0r9uD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contents\n",
        "\n",
        "Section 1 - Introduction to Tokenization\n",
        "\n",
        "Section 2 - Normalization and Pre-Tokenization\n",
        "\n",
        "Section 3 - Subword Tokenization Methods\n",
        "\n",
        "Section 4 - Tokenizers in Python Libraries\n",
        "\n",
        "Section 5 - Conclusion\n",
        "\n",
        "Section 6 - Glossary\n",
        "\n",
        "Section 7 - Further Reading"
      ],
      "metadata": {
        "id": "R8TIwUSpr_lG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependancies"
      ],
      "metadata": {
        "id": "U0ICfMvUAlMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tokenizers"
      ],
      "metadata": {
        "id": "RbFg5q4RApkJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align=\"center\">Section 1 - Introduction to Tokenization</h2>"
      ],
      "metadata": {
        "id": "MhGtXpLh5-EP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 - Overview of Tokenizers\n",
        "\n",
        "Natural language problems concern textual data, which cannot be immediately understood by a machine. For computers to process language, they must first convert the text into a numerical form. This process is carried out in two stages by a model called the **tokenizer**. The tokenizer first takes the text and divides it into smaller pieces, be that words, parts of words, or individual characters. These smaller pieces of text are called **tokens**. The Stanford NLP Group [1] defines tokens more rigorously as:\n",
        "\n",
        "> an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing.\n",
        "\n",
        "Once the tokenizer has divided the text into tokens, each token can be assigned a number. An example of this might be that the word 'cat' is assigned the value '15', and so every 'cat' token in the input text will be represented by the number 15. The process of replacing the textual tokens with a number representation is called **encoding**. Similarly, the process of converting encoded tokens back into text is called **decoding**.\n",
        "\n",
        "&nbsp;"
      ],
      "metadata": {
        "id": "3NGYcmq8ONl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 - Different Tokenization Methods\n",
        "\n",
        "There are three main different methods for dividing text into tokens:\n",
        "\n",
        "* Word-based\n",
        "* Character-based\n",
        "* Subword-based\n",
        "\n",
        "The following cells give an overview of each of these methods, along with some pros and cons for each.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "Dx6la0GJQTIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 - Overview of Word-Based Tokenization Methods\n",
        "\n",
        "**Word-based tokenization** is perhaps the most simple of the three methods described above. Here, the tokenizer will divide sentences into words by splitting on each space character (sometimes called 'whitespace-based tokenization), or by a similar set of rules (such as punctuation-based tokenization, treebank tokenization, etc) [2].\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "For example, the sentence:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "`This sentence is a great, interesting sentence!`\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "could be split on whitespace characters to give:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "`['This', 'sentence', 'is', 'a', 'great,', 'interesting', 'sentence!']`\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "or by split on both punctuation and spaces to give:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "`['This', 'sentence', 'is', 'a', 'great', ',', 'interesting', 'sentence', '!']`\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "From this simple example, it is clear that the rules used to determine the split are important, since the first split gives the potentially rare token 'sentence!', while the second split gives the two, less-rare tokens 'sentence' and '!'. Care should be taken not to remove punctuation altogether, as they can carry very specific meanings. An example of this is the apostrophe, which can distinguish between the plural and possessive form of words. For example \"book's\" refers to some property of a book, as in \"the book's spine is damaged\", and \"books\" refers to many books, as in \"the books are damaged\".\n",
        "\n",
        "Once the tokens have been generated, each can be assigned a number. The next time that a token is generated that the tokenizer has already seen, it can simply assign the number that was assigned earlier. For example, if the token 'great' is assigned the value 1 in the sentence above, and the tokenizer is given another sentence that contains the word 'great', this second instance (and all subsequent instances) of the word 'great' will also be assigned the value of 1 [3].\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "-FBOHcuWXnWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 - Pros and Cons of Word-Based Tokenization Methods\n",
        "\n",
        "The tokens produced in the word-based method contain a high degree of information, since each token contains semantic and contextual information. However, one of the largest drawbacks with this method is that very similar words are treated as completely separate tokens. For example, the connection between 'cat' and 'cats' would be non-existent, and so these would be treated as separate words. This becomes a problem in large-scale applications that contain many words, as the possible number of tokens in the model's **vocabulary** (the total set of tokens a model has seen) can grow very large. English has around 170,000 words, and so including various grammatical forms like plurals and past-tense for each word can lead to what is known as the **exploding vocabulary problem**. An example of this is the TransformerXL tokenizer which uses whitespace-based splitting. This led to a vocabulary size of over 250,000 [4].\n",
        "\n",
        "One way to combat this is by enforcing a hard limit on the number of tokens the model can learn (e.g. 10,000). This would classify any word outside of the 10,000 most frequent tokens as **out-of-vocabulary** (OOV), and would assign the token value of 'UNKNOWN' instead of a number value. This causes performance to suffer in cases where many unknown words are present, but may be a suitable compromise if the data contains mostly common words. [3]\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Summary of Pros:**\n",
        "\n",
        "* Simple\n",
        "\n",
        "* High degree of information stored in each token\n",
        "\n",
        "* Can limit vocabulary size which works well with datasets containing mostly common words\n",
        "\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Summary of Cons:**\n",
        "\n",
        "* Separate tokens are created for similar words (e.g. 'cat' and 'cats')\n",
        "\n",
        "* Can result in very large vocabulary\n",
        "\n",
        "* Limiting vocabulary can significantly degrade performance on datasets with many uncommon words\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "CyAfy9NCYPjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 - Overview of Character-Based Tokenization Methods\n",
        "\n",
        "**Character-based tokenization** splits text on each character, including letters, numbers, and special characters such as punctuation. This greatly reduces the vocabulary size, to the point where the English language can be represented with a vocabulary of around 256 tokens, instead of the 170,000+ needed with word-based approaches [5]. Even east Asian languages such as Chinese and Japanese can see a significant reduction in their vocabulary size, despite containing thousands of unqiue characters in their writing systems.\n",
        "\n",
        "In a character-based tokenizer, the following sentence:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "`This sentence is a great, interesting sentence!`\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "would be converted to:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "`['T', 'h', 'i', 's', ' ', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', ' ', 'i', 's', ' ', 'a', ' ', 'g', 'r', 'e', 'a', 't', ',', ' ,'i',\n",
        "'n', 't', 'e', 'r', 'e', 's', 't', 'i', 'n', 'g', ' ', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', '!'`]\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "a8BJkRg-ZL2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6 - Pros and Cons of Character-Based Tokenization Methods\n",
        "\n",
        "Character-based approaches result in a much smaller vocabulary size when compared to word-based methods, and also result in much fewer out-of-vocabulary tokens. This even allows misspelled words to be tokenized (albeit differently than the correct form of the word), rather than being removed immediately due to the frequency-based vocabulary limit.\n",
        "\n",
        "However there are a number of drawbacks with this approach too. Firstly, the information stored in a single token produced with a character-based method is very low. This is because unlike the tokens in the word-based method, no semantic or contextual meaning is captured (especially in languages with alphabet-based writing systems like English). Finally, the size of the tokenized input that can be fed into a language model is limited with this approach, since many numbers are required to the input text.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Summary of Pros:**\n",
        "\n",
        "* Smaller vocabulary size\n",
        "\n",
        "* Does not remove misspelled words\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Summary of Cons:**\n",
        "\n",
        "* Low information stored in each token, little-to-no contextual or semantic meaning (especially with alphabet-based writing systems)\n",
        "\n",
        "* Size of input to language models is limited since the output of the tokenizer requires many more numbers to tokenize text (when compared to a word-based approach)\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "faTIXhEEaXHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7 - Overview of Subword-Based Tokenization\n",
        "\n",
        "**Subword-based tokenization** aims to achieve the benefits of both word-based and character-based methods while minimising their downsides. Subword-based methods take a middle ground by splitting text within words in an attempt to create tokens with semantic meaning, even if they are not full words. For example, the tokens `ing` and `ed` carry grammatical meaning, although they are not words in themselve.\n",
        "\n",
        "This method results in a vocabulary size that is smaller than those found in word-based methods, but larger than those found in character-based methods. The same is also true for the amount of information stored within each token, which is also in between the tokens generated by the previous two methods. The subword approach uses the following two guidelines:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "* Frequently used words should not be split into subwords, but rather be stored as entire tokens\n",
        "\n",
        "* Infrequently used words should be split into subwords\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Splitting only the infrequently used words gives a chance for the conjugations, plural forms etc to be decomposed into their constiutent parts, while preserving the relationship between tokens. For example `cat` might be a very common word in the dataset, but `cats` might be less common. For this reason, `cats` would be split into `cat` and `s`, where `cat` is now assigned the same value as every other `cat` token, and `s` is assigned a different value, which can encode the meaning of plurality. Another example would be the word `tokenization`, which can be split into the root word `token` and the suffix `ization`. This method can therefore preserve syntactic and semantic similarity [6]. For these reasons, subword-based tokenizers are very commonly used in NLP models today.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "qcKt34SyePPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align=\"center\">Section 2 - Normalization and Pre-Tokenization</h2>\n",
        "\n"
      ],
      "metadata": {
        "id": "HmAsX58msaTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 - Overview of the Tokenization Pipeline\n",
        "\n",
        "The tokenization process requires some pre-processing and post-processing steps, that in all comprise the **tokenization pipeline**. This describes the entires series of actions that are taken to convert raw text into tokens. The steps of this pipeline are:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "* Normalization\n",
        "\n",
        "* Pre-tokenization\n",
        "\n",
        "* Model\n",
        "\n",
        "* Post-processing\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "where the tokenization method (be that subword-based, character-based etc) takes place in the model step. [7] This section will cover each of these steps for a tokenizer that uses a subword-based tokenization approach.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**IMPORTANT NOTE:** all the steps of the tokenization pipeline are handled for the user automaticallly when using a tokenizer from libraries such as the Hugging Face `tokenizers` and `transformers` libraries. The entire pipeline is performed by a single object called the Tokenizer. The cells in this section dive into the inner workings of the code the most users do not need to handle manually when working with NLP tasks. Later in the notebook, the steps to customise the base tokenizer class in the `tokenizers` library is also presented, so that a tokenizer can be purpose-built for a specific task if required.\n",
        "\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "0_rmfFZ6h4ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 - Normalization Methods\n",
        "\n",
        "**Normalization** is the process of cleaning text before it is split into tokens. This includes converting each character to lowercase, removing accents from characters (e.g. 'é' becomes 'e'), removing unnecessary whitespace, and so on. For example, the string `ThÍs is  áN ExaMPlé     sÉnteNCE` becomes `this is an example sentence` after normalization. Different normalizers will perform different steps, which can be useful depending on the use-case. For example, in some situations the casing or accents might need to be preserved. Depending on the normalizer chosen, different effects can be achieved at this stage.\n",
        "\n",
        "The Hugging Face `tokenizers.normalizers` package contains several basic normalizers that are used by different tokenizers are part of larger models. The base normalizer class can be imported directly however to investigate how they work. Below shows the NFC unicode, Lowercase, and BERT normalizers. These show the following effects on the example sentence:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "* **NFC:** Does not convert casing or remove accents\n",
        "* **Lower:** Converts casing but does not remove accents\n",
        "* **BERT:** Converts casing and removes accents\n",
        "\n",
        "&nbsp;"
      ],
      "metadata": {
        "id": "GbhH1mlT9lhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.normalizers import NFC, Lowercase, BertNormalizer\n",
        "\n",
        "# Text to normalize\n",
        "example_sentence = 'ThÍs is  áN ExaMPlé     sÉnteNCE'\n",
        "\n",
        "# Instantiate normalizer objects\n",
        "NFCNorm = NFC()\n",
        "LowercaseNorm = Lowercase()\n",
        "BertNorm = BertNormalizer()\n",
        "\n",
        "# Normalize the text\n",
        "print(f'NFC:   {NFCNorm.normalize_str(example_sentence)}')\n",
        "print(f'Lower: {LowercaseNorm.normalize_str(example_sentence)}')\n",
        "print(f'BERT:  {BertNorm.normalize_str(example_sentence)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJmRZzqaHaz-",
        "outputId": "6e81d441-4cd0-4ed0-8de3-25a005191da8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NFC:   ThÍs is  áN ExaMPlé     sÉnteNCE\n",
            "Lower: thís is  án examplé     séntence\n",
            "BERT:  this is  an example     sentence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "&nbsp;\n",
        "\n",
        "The normalizers above are used in tokenizer models which can be imported from the Hugging Face `transformers` library. The cell below shows that the normalizers can be accessed using dot notation via `Tokenizer.backend_tokenizer.normalizer`. Some comparisons are shown between the tokenizsers to highlight the different normalization methods that are used. Note that in these examples, only the FNet normalizer removes unncessary whitespace.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "Ym3WMKs0MX97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import FNetTokenizerFast, CamembertTokenizerFast, BertTokenizerFast\n",
        "\n",
        "# Text to normalize\n",
        "example_sentence = 'ThÍs is  áN ExaMPlé     sÉnteNCE'\n",
        "\n",
        "# Instatiate tokenizers\n",
        "FNetTokenizer = FNetTokenizerFast.from_pretrained('google/fnet-base')\n",
        "CamembertTokenizer = CamembertTokenizerFast.from_pretrained('camembert-base')\n",
        "BertTokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Normalize the text\n",
        "print(f'FNet Output:      {FNetTokenizer.backend_tokenizer.normalizer.normalize_str(example_sentence)}')\n",
        "print(f'CamemBERT Output: {CamembertTokenizer.backend_tokenizer.normalizer.normalize_str(example_sentence)}')\n",
        "print(f'BERT Output:      {BertTokenizer.backend_tokenizer.normalizer.normalize_str(example_sentence)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2THnB6kLAEzE",
        "outputId": "b2f8dd78-8167-4e56-e4ff-a72d68b9ce14"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FNet Output:      ThÍs is áN ExaMPlé sÉnteNCE\n",
            "CamemBERT Output: ThÍs is  áN ExaMPlé     sÉnteNCE\n",
            "BERT Output:      this is  an example     sentence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 - Pre-Tokenization Methods\n",
        "\n",
        "The **pre-tokenization** step is the first splitting of the raw text in the tokenization pipeline. The split is performed to give an upper bound to what the final tokens could be at the end of the pipeline. That is, a sentence can be split into words in the pre-tokenization step, then in the model step some of these words may be split further according to the tokenization method (e.g. subword-based). So the pre-tokenized text represents the largest possible tokens that could still remain after tokenization.\n",
        "\n",
        "Just like with normalization, there are several ways that this step can be performed. For example, a sentence can be split based on every space, every space and some punctuation, or every space and every punctuation.\n",
        "\n",
        "The cell below shows a comparison between the basic `Whitespacesplit` pre-tokenizer, and slightly more complex `BertPreTokenizer` from the Hugging Face `tokenizers.pre_tokenizers` package. The output of the whitespace pre-tokenizer leaves the punctuation in-tact, and still attached to the neighbouring words. For example `includes:` is treated as a single word in this case. Whereas the BERT pre-tokenizer treats punctuation as individual words [8].\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "nWqnWe9ANT2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.pre_tokenizers import WhitespaceSplit, BertPreTokenizer\n",
        "\n",
        "# Text to pre-tokenize - written in lowercase with no unnecessary whitespace to simulate normalization step\n",
        "example_sentence = \"this sentence's content includes: characters, spaces, and punctuation.\"\n",
        "\n",
        "# Define helper function to display pre-tokenized output\n",
        "def print_pretokenized_str(pre_tokens):\n",
        "    for pre_token in pre_tokens:\n",
        "        print(f'\"{pre_token[0]}\", ', end='')\n",
        "\n",
        "# Instantiate pre-tokenizers\n",
        "wss = WhitespaceSplit()\n",
        "bpt = BertPreTokenizer()\n",
        "\n",
        "# Pre-tokenize the text\n",
        "print('Whitespace Pre-Tokenizer:')\n",
        "print_pretokenized_str(wss.pre_tokenize_str(example_sentence))\n",
        "\n",
        "print('\\n\\nBERT Pre-Tokenizer:')\n",
        "print_pretokenized_str(bpt.pre_tokenize_str(example_sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZkMWhcBjRTD",
        "outputId": "9410dca7-bf39-4dd7-c688-0526654fcef8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Pre-Tokenizer:\n",
            "\"this\", \"sentence's\", \"content\", \"includes:\", \"characters,\", \"spaces,\", \"and\", \"punctuation.\", \n",
            "\n",
            "BERT Pre-Tokenizer:\n",
            "\"this\", \"sentence\", \"'\", \"s\", \"content\", \"includes\", \":\", \"characters\", \",\", \"spaces\", \",\", \"and\", \"punctuation\", \".\", "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "&nbsp;\n",
        "\n",
        "Just as with the normalization methods, you can call the pre-tokenization methods directly from common tokenizers such as the GPT-2 and ALBERT (A Lite BERT) tokenizers. These take a slightly different approach to the standard BERT pre-tokenizer shown above, in that space characters are not removed when splitting the tokens. Instead, they are replaced with special characters that represent where the space was. This has the advantage in that the space characters can be ignored when processing further, but the original sentence can be retrieved if required. The GPT-2 model uses the `Ġ` character, which features a capital 'G' with a dot above. The ALBERT models uses an underscore character.\n",
        "\n",
        "&nbsp;"
      ],
      "metadata": {
        "id": "vDy0lC50qLv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Text to pre-tokenize - written in lowercase with no unnecessary whitespace to simulate normalization step\n",
        "example_sentence = \"this sentence's content includes: characters, spaces, and punctuation.\"\n",
        "\n",
        "# Instatiate the pre-tokenizers\n",
        "GPT2_PreTokenizer = AutoTokenizer.from_pretrained('gpt2').backend_tokenizer.pre_tokenizer\n",
        "Albert_PreTokenizer = AutoTokenizer.from_pretrained('albert-base-v1').backend_tokenizer.pre_tokenizer\n",
        "\n",
        "# Pre-tokenize the text\n",
        "print('GPT-2 Pre-Tokenizer:')\n",
        "print_pretokenized_str(GPT2_PreTokenizer.pre_tokenize_str(example_sentence))\n",
        "print('\\n\\nALBERT Pre-Tokenizer:')\n",
        "print_pretokenized_str(Albert_PreTokenizer.pre_tokenize_str(example_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9KHyQ3cpr4C",
        "outputId": "28144d89-ae5b-46e9-ebce-49971681c461"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-2 Pre-Tokenizer:\n",
            "\"this\", \"Ġsentence\", \"'s\", \"Ġcontent\", \"Ġincludes\", \":\", \"Ġcharacters\", \",\", \"Ġspaces\", \",\", \"Ġand\", \"Ġpunctuation\", \".\", \n",
            "\n",
            "ALBERT Pre-Tokenizer:\n",
            "\"▁this\", \"▁sentence's\", \"▁content\", \"▁includes:\", \"▁characters,\", \"▁spaces,\", \"▁and\", \"▁punctuation.\", "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "&nbsp;\n",
        "\n",
        "The cells above show the output of pre-tokenization is a compact format that nicely fits on the screen and removes some of the additional information generated. Below shows the results of a BERT pre-tokenization step on the same example sentence without any of the modifications to the outputs as was shown above. The object returned is a Python list containing tuples. Each tuple corresponds to a pre-token, where the first element is the pre-token string, and the second element is a tuple containing the index for the start and end of the string in the original input text. Note that the starting index of the string is inclusive, and the ending index is exclusive.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "lvUONlVKsCaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.pre_tokenizers import WhitespaceSplit, BertPreTokenizer\n",
        "\n",
        "# Text to pre-tokenize - written in lowercase with no unnecessary whitespace to simulate normalization step\n",
        "example_sentence = \"this sentence's content includes: characters, spaces, and punctuation.\"\n",
        "\n",
        "# Instantiate pre-tokenizer\n",
        "bpt = BertPreTokenizer()\n",
        "\n",
        "# Pre-tokenize the text\n",
        "bpt.pre_tokenize_str(example_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-TmHxqLsVH4",
        "outputId": "a1e75fa6-c760-412b-85de-b52a0c47fb0a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('this', (0, 4)),\n",
              " ('sentence', (5, 13)),\n",
              " (\"'\", (13, 14)),\n",
              " ('s', (14, 15)),\n",
              " ('content', (16, 23)),\n",
              " ('includes', (24, 32)),\n",
              " (':', (32, 33)),\n",
              " ('characters', (34, 44)),\n",
              " (',', (44, 45)),\n",
              " ('spaces', (46, 52)),\n",
              " (',', (52, 53)),\n",
              " ('and', (54, 57)),\n",
              " ('punctuation', (58, 69)),\n",
              " ('.', (69, 70))]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align=\"center\">Section 3 - Subword Tokenization Methods</h2>\n",
        "\n"
      ],
      "metadata": {
        "id": "IGEFQnFsHZmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 - Subword Tokenization Methods\n",
        "\n",
        "The model step of the tokenization pipeline is where the tokenization method comes into play. As described earlier, the options here are: word-based, character-based, and subword-based. Subword-based are generally favoured, since these methods were designed to overcome the limitations of the word-based and character-based approaches.\n",
        "\n",
        "For transformer models, there are three tokenizer methods that are commonly used to implement subword-based tokenization. These include:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "* Byte Pair Encoding (BPE)\n",
        "\n",
        "* WordPiece\n",
        "\n",
        "* Unigram\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Each of these use slightly different techniques to split the less frequent words into smaller tokens, which are laid out in this section. In addition, implementations of the BPE and WordPiece algorithms are also shown to help highlight some of the similarities and differences between the approaches.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "kb62ki1fOLKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 - Byte Pair Encoding (BPE) Tokenization\n",
        "\n",
        "The **Byte Pair Encoding** algorithm is a commonly-used tokenizer that is found in many transformer models such as Open AI's GPT and GPT-2 models, BART, and many others [9-10]. It was originally designed as a text compression algorithm, but has been found to work very well in tokenization tasks for language models. The BPE algorithm decomposes a string of text into subword units that appear frequently in a reference corpus (the text used to train the tokenization model) [11]. The BPE model is trained as follows:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 1) Construct the Corpus**\n",
        "\n",
        "The input text is given to the normalization and pre-tokenization models to create clean words. The words are then given to the BPE model, which determines the frequency of each word, and stores this number alongside the word in a list called the **corpus**.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 2) Construct the Vocabulary**\n",
        "\n",
        "The words from the corpus are then broken down individual characters and are added to an empty list called the vocabulary. The algorithm will iteratively add to this vocabulary every time it determines which character pairs can be merged together.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 3) Find the Frequency of Character Pairs**\n",
        "\n",
        "The frequency of character pairs is then recorded for each word in the corpus. For example, the words `cats` will have the character pairs `ca`, `at`, and `ts`. All the words are examined in this way, and contribute to a global frequency counter. So any instance of `ca` found in any of the tokens will increase the frequency counter for the `ca` pair.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 4) Create a Merging Rule**\n",
        "\n",
        "When the frequency for each character pair is known, the most frequent character pair is added to the vocabulary. The vocabulary now consists of every individual letter in the tokens, plus the most frequent character pair. This also gives a merging rule that the model can use. For example, if the model learns that `ca` is the most frequent character pair, it has learned that all adjacent instances of `c` and `a` in the corpus can be merged to give `ca`. This can now be treated as a single character `ca` for the remainder of the steps.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 5) Repeat Steps 3 and 4**\n",
        "\n",
        "Steps 3 and are then repeated, finding more merging rules, and adding more character pairs to the vocabulary. This process continues until the vocabulary size reaches a target size specified at the beginning of the training.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Now that the BPE algorithm has been trained (i.e. now that all have the merging rules have been found), the model can be used to tokenize any text by first splitting each of the words on every character, and then merging according to the merge rules.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "rs5-JG_jthF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 - Implementation of BPE in Python\n",
        "\n",
        "Below shows a vanilla Python implementation of the BPE algorithm, following the steps outlined above. The following cells show the model trained on a toy dataset, and tested on some example words.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "eDMZNTmEFZUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TargetVocabularySizeError(Exception):\n",
        "    def __init__(self, message):\n",
        "        super().__init__(message)\n",
        "\n",
        "class BPE:\n",
        "    '''An implementation of the Byte Pair Encoding tokenizer.'''\n",
        "\n",
        "    def calculate_frequency(self, words):\n",
        "        ''' Calculate the frequency for each word in a list of words.\n",
        "\n",
        "            Take in. list of words stored as strings and return a list of tuples\n",
        "            where each tuple contains a string from the words list, and an\n",
        "            integer representing its frequency count in the list.\n",
        "\n",
        "            Args:\n",
        "                words (list):  A list of words (strings) in any order.\n",
        "\n",
        "            Returns:\n",
        "                corpus (list[tuple(str, int)]):\n",
        "                               A list of tuples where the first element is a\n",
        "                               string of a word in the words list, and the\n",
        "                               second element is an integer representing the\n",
        "                               frequency of the word in the list.\n",
        "        '''\n",
        "        freq_dict = dict()\n",
        "\n",
        "        for word in words:\n",
        "            if word not in freq_dict:\n",
        "                freq_dict[word] = 1\n",
        "            else:\n",
        "                freq_dict[word] += 1\n",
        "\n",
        "        corpus = [(word, freq_dict[word]) for word in freq_dict.keys()]\n",
        "\n",
        "        return corpus\n",
        "\n",
        "\n",
        "    def create_merge_rule(self, corpus):\n",
        "        ''' Create a merge rule and add it to the self.merge_rules list.\n",
        "\n",
        "            Args:\n",
        "                corpus (list[tuple(list, int)]):\n",
        "                               A list of tuples where the first element is a\n",
        "                               list of a word in the words list (where the\n",
        "                               elements are the individual characters (or\n",
        "                               subwords in later iterations) of the\n",
        "                               word), and the second element is an integer\n",
        "                               representing the frequency of the word in the\n",
        "                               list.\n",
        "\n",
        "            Returns:\n",
        "                None\n",
        "        '''\n",
        "        pair_frequencies = self.find_pair_frequencies(corpus)\n",
        "        most_frequent_pair = max(pair_frequencies, key=pair_frequencies.get)\n",
        "        self.merge_rules.append(most_frequent_pair.split(','))\n",
        "        self.vocabulary.append(most_frequent_pair)\n",
        "\n",
        "\n",
        "    def create_vocabulary(self, words):\n",
        "        ''' Create a list of every unique character in a list of words.\n",
        "\n",
        "            Args:\n",
        "                words (list): A list of strings containing the words of the\n",
        "                              input text.\n",
        "\n",
        "            Returns:\n",
        "                vocabulary (list): A list of every unique character in the list\n",
        "                                   of input words.\n",
        "        '''\n",
        "        vocabulary = list(set(''.join(words)))\n",
        "        return vocabulary\n",
        "\n",
        "    def find_pair_frequencies(self, corpus):\n",
        "        ''' Find the frequency of each character pair in the corpus.\n",
        "\n",
        "            Loop through the corpus and calculate the frequency of each pair\n",
        "            of adjacent characters across every word. Return a dictionary of\n",
        "            each character pair as the keys and the corresponding frequency as\n",
        "            the values.\n",
        "\n",
        "            Args:\n",
        "                corpus (list[tuple(list, int)]):\n",
        "                               A list of tuples where the first element is a\n",
        "                               list of a word in the words list (where the\n",
        "                               elements are the individual characters (or\n",
        "                               subwords in later iterations) of the\n",
        "                               word), and the second element is an integer\n",
        "                               representing the frequency of the word in the\n",
        "                               list.\n",
        "\n",
        "            Returns:\n",
        "                pair_freq_dict (dict): A dictionary where the keys are the\n",
        "                                       character pairs from the input corpus and\n",
        "                                       the values are an integer representing\n",
        "                                       the frequency of the pair in the corpus.\n",
        "        '''\n",
        "        pair_freq_dict = dict()\n",
        "\n",
        "        for word, word_freq in corpus:\n",
        "            for idx in range(len(word)-1):\n",
        "\n",
        "                char_pair = f'{word[idx]},{word[idx+1]}'\n",
        "\n",
        "                if char_pair not in pair_freq_dict:\n",
        "                    pair_freq_dict[char_pair] = word_freq\n",
        "                else:\n",
        "                    pair_freq_dict[char_pair] += word_freq\n",
        "\n",
        "        return pair_freq_dict\n",
        "\n",
        "\n",
        "    def get_merged_chars(self, char_1, char_2):\n",
        "        ''' Merge the highest score pair and return to the self.merge method.\n",
        "\n",
        "            This method is abstracted so that the BPE class can be used as the\n",
        "            base class for other Tokenizers, and so the merging method can be\n",
        "            easily overwritten. For example, in the BPE algorithm the characters\n",
        "            can simply be concatenated and returned. However in the WordPiece\n",
        "            algorithm, the # symbols must first be stripped.\n",
        "\n",
        "            Args:\n",
        "                char_1 (str): The first character in the highest-scoring pair.\n",
        "                char_2 (str): The second character in the highest-scoring pair.\n",
        "\n",
        "            Returns:\n",
        "                merged_chars (str): Merged characters.\n",
        "        '''\n",
        "        merged_chars = char_1 + char_2\n",
        "        return merged_chars\n",
        "\n",
        "\n",
        "    def initialise_corpus(self, words):\n",
        "        ''' Split each word into characters and count the word frequency.\n",
        "\n",
        "            Split each word in the input word list on every character. For each\n",
        "            word, store the split word in a list as the first element inside a\n",
        "            tuple. Store the frequency count of the word as an integer as the\n",
        "            second element of the tuple. Create a tuple for every word in this\n",
        "            fashion and store the tuples in a list called 'corpus', then return\n",
        "            then corpus list.\n",
        "\n",
        "            Args:\n",
        "                None\n",
        "\n",
        "            Returns:\n",
        "                corpus (list[tuple(list, int)]):\n",
        "                               A list of tuples where the first element is a\n",
        "                               list of a word in the words list (where the\n",
        "                               elements are the individual characters of the\n",
        "                               word), and the second element is an integer\n",
        "                               representing the frequency of the word in the\n",
        "                               list.\n",
        "        '''\n",
        "        corpus = self.calculate_frequency(words)\n",
        "        corpus = [([*word], freq) for (word, freq) in corpus]\n",
        "        return corpus\n",
        "\n",
        "\n",
        "    def merge(self, corpus):\n",
        "        ''' Loop through the corpus and perform the latest merge rule.\n",
        "\n",
        "            Args:\n",
        "                corpus (list[tuple(list, int)]):\n",
        "                            A list of tuples where the first element is a\n",
        "                            list of a word in the words list (where the\n",
        "                            elements are the individual characters (or\n",
        "                            subwords in later iterations) of the\n",
        "                            word), and the second element is an integer\n",
        "                            representing the frequency of the word in the\n",
        "                            list.\n",
        "\n",
        "            Returns:\n",
        "                new_corpus (list[tuple(list, int)]):\n",
        "                            A modified version of the input argument where the\n",
        "                            most recent merge rule has been applied to merge\n",
        "                            the most frequent adjacent characters.\n",
        "        '''\n",
        "        merge_rule = self.merge_rules[-1]\n",
        "        new_corpus = []\n",
        "\n",
        "        for word, word_freq in corpus:\n",
        "            new_word = []\n",
        "            idx = 0\n",
        "\n",
        "            while idx < len(word):\n",
        "                # If a merge pattern has been found\n",
        "                if (len(word) != 1) and (word[idx] == merge_rule[0]) and\\\n",
        "                (word[idx+1] == merge_rule[1]):\n",
        "\n",
        "                    new_word.append(self.get_merged_chars(word[idx],word[idx+1]))\n",
        "                    idx += 2\n",
        "                # If a merge patten has not been found\n",
        "                else:\n",
        "                    new_word.append(word[idx])\n",
        "                    idx += 1\n",
        "\n",
        "            new_corpus.append((new_word, word_freq))\n",
        "\n",
        "        return new_corpus\n",
        "\n",
        "\n",
        "    def train(self, words, target_vocab_size):\n",
        "        ''' Train the model.\n",
        "\n",
        "            Args:\n",
        "                words (list[str]): A list of words to train the model on.\n",
        "\n",
        "                target_vocab_size (int): The number of words in the vocabulary\n",
        "                                         to be used as the stopping condition\n",
        "                                         when training.\n",
        "\n",
        "            Returns:\n",
        "                None.\n",
        "        '''\n",
        "        self.words = words\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "        self.corpus = self.initialise_corpus(self.words)\n",
        "        self.corpus_history = [self.corpus]\n",
        "        self.vocabulary = self.create_vocabulary(self.words)\n",
        "        self.vocabulary_size = len(self.vocabulary)\n",
        "        self.merge_rules = []\n",
        "\n",
        "        # Iteratively add vocabulary until the target vocabulary size is reached\n",
        "        if len(self.vocabulary) > self.target_vocab_size:\n",
        "            raise TargetVocabularySizeError(f'Error: Target vocabulary size \\\n",
        "            must be greater than the initial vocabulary size \\\n",
        "            ({len(self.vocabulary)})')\n",
        "\n",
        "        else:\n",
        "            while len(self.vocabulary) < self.target_vocab_size:\n",
        "                try:\n",
        "                    self.create_merge_rule(self.corpus)\n",
        "                    self.corpus = self.merge(self.corpus)\n",
        "                    self.corpus_history.append(self.corpus)\n",
        "\n",
        "                # If no further merging is possible\n",
        "                except ValueError:\n",
        "                    print('Exiting: No further merging is possible')\n",
        "                    break\n",
        "\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        ''' Take in some text and return a list of tokens for that text.\n",
        "\n",
        "            Args:\n",
        "                text (str): The text to be tokenized.\n",
        "\n",
        "            Returns:\n",
        "                tokens (list): The list of tokens created from the input text.\n",
        "        '''\n",
        "        tokens = [*text]\n",
        "\n",
        "        for merge_rule in self.merge_rules:\n",
        "\n",
        "            new_tokens = []\n",
        "            idx = 0\n",
        "\n",
        "            while idx < len(tokens):\n",
        "                # If a merge pattern has been found\n",
        "                if (len(tokens) != 1) and (tokens[idx] == merge_rule[0]) and (tokens[idx+1] == merge_rule[1]):\n",
        "\n",
        "                    new_tokens.append(self.get_merged_chars(tokens[idx],\n",
        "                                                            tokens[idx+1]))\n",
        "                    idx += 2\n",
        "                # If a merge patten has not been found\n",
        "                else:\n",
        "                    new_tokens.append(tokens[idx])\n",
        "                    idx += 1\n",
        "\n",
        "            tokens = new_tokens\n",
        "\n",
        "        return tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "D1RqTPzwuoit"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 - Training the BPE Algorithm with a Toy Dataset\n",
        "\n",
        "The BPE algorithm is trained below on a toy dataset that contains some words about cats. The goal of the tokenizer is to determine the most useful, meaningful subunits of the words in the dataset to be used as tokens. From inspection, it is clear that units such as 'cat', 'eat' and 'ing' would be useful subunits.\n",
        "\n",
        "Running the tokenizer with a target vocabulary size of 21 (which only requires 5 merges) is enough for the tokenizer to capture all the desired subunits mentioned above. With a larger dataset, the target vocabulary would be much higher, but this shows how powerful the BPE tokenizer can be.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "JL6-Q9FoFpgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training set\n",
        "words = ['cat', 'cat', 'cat', 'cat', 'cat',\n",
        "         'cats', 'cats',\n",
        "         'eat', 'eat', 'eat', 'eat', 'eat', 'eat', 'eat', 'eat', 'eat', 'eat',\n",
        "         'eating', 'eating', 'eating',\n",
        "         'running', 'running',\n",
        "         'jumping',\n",
        "         'food', 'food', 'food', 'food', 'food', 'food']\n",
        "\n",
        "# Instantiate the tokenizer\n",
        "bpe = BPE()\n",
        "bpe.train(words, 21)\n",
        "\n",
        "# Print out the corpus at each stage of the process, as well as the merge rule used\n",
        "print(f'INITIAL CORPUS:\\n{bpe.corpus_history[0]}\\n')\n",
        "for rule, corpus in list(zip(bpe.merge_rules, bpe.corpus_history[1:])):\n",
        "    print(f'NEW MERGE RULE: Combine \"{rule[0]}\" and \"{rule[1]}\"')\n",
        "    print(corpus, end='\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6QKvD3HCjpS",
        "outputId": "1d875c37-8d25-405f-eb2b-0e4548b6532b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INITIAL CORPUS:\n",
            "[(['c', 'a', 't'], 5), (['c', 'a', 't', 's'], 2), (['e', 'a', 't'], 10), (['e', 'a', 't', 'i', 'n', 'g'], 3), (['r', 'u', 'n', 'n', 'i', 'n', 'g'], 2), (['j', 'u', 'm', 'p', 'i', 'n', 'g'], 1), (['f', 'o', 'o', 'd'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"a\" and \"t\"\n",
            "[(['c', 'at'], 5), (['c', 'at', 's'], 2), (['e', 'at'], 10), (['e', 'at', 'i', 'n', 'g'], 3), (['r', 'u', 'n', 'n', 'i', 'n', 'g'], 2), (['j', 'u', 'm', 'p', 'i', 'n', 'g'], 1), (['f', 'o', 'o', 'd'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"e\" and \"at\"\n",
            "[(['c', 'at'], 5), (['c', 'at', 's'], 2), (['eat'], 10), (['eat', 'i', 'n', 'g'], 3), (['r', 'u', 'n', 'n', 'i', 'n', 'g'], 2), (['j', 'u', 'm', 'p', 'i', 'n', 'g'], 1), (['f', 'o', 'o', 'd'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"c\" and \"at\"\n",
            "[(['cat'], 5), (['cat', 's'], 2), (['eat'], 10), (['eat', 'i', 'n', 'g'], 3), (['r', 'u', 'n', 'n', 'i', 'n', 'g'], 2), (['j', 'u', 'm', 'p', 'i', 'n', 'g'], 1), (['f', 'o', 'o', 'd'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"i\" and \"n\"\n",
            "[(['cat'], 5), (['cat', 's'], 2), (['eat'], 10), (['eat', 'in', 'g'], 3), (['r', 'u', 'n', 'n', 'in', 'g'], 2), (['j', 'u', 'm', 'p', 'in', 'g'], 1), (['f', 'o', 'o', 'd'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"in\" and \"g\"\n",
            "[(['cat'], 5), (['cat', 's'], 2), (['eat'], 10), (['eat', 'ing'], 3), (['r', 'u', 'n', 'n', 'ing'], 2), (['j', 'u', 'm', 'p', 'ing'], 1), (['f', 'o', 'o', 'd'], 6)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 - Tokenizing using BPE\n",
        "\n",
        "Now that the BPE algorithm has been trained with this very small dataset, it can be used to tokenize some example words. The cell below shows the tokenizer being used to tokenize some words it has seen before, and some it has not. Notice that the tokenizer has learned the verb ending `ing`, and so can split these characters into a token, even in words it has never seen before. In the case of `eating`, the training data contained the word `eat`, and so the model was able to learn that this is a significant token all by itself. However, the model has never seen the words `run` and `ski`, and so does not successfully tokenize these as root words. This highlights the important of a wide and varied training set when training a tokenizer.\n",
        "\n",
        "&nbsp;"
      ],
      "metadata": {
        "id": "V4YmvMwfmJzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(bpe.tokenize('eating'))\n",
        "print(bpe.tokenize('running'))\n",
        "print(bpe.tokenize('skiing'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYlEqanSmhAr",
        "outputId": "88235980-3eeb-446d-9313-d0984e0f3758"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['eat', 'ing']\n",
            "['r', 'u', 'n', 'n', 'ing']\n",
            "['s', 'k', 'i', 'ing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 - The Byte-Level BPE Trick\n",
        "\n",
        "BPE tokenizers can only recognise characters that have appeared in the training data. For example, the training data above only contained the characters needed to talk about cats, which happened to not require a `z`. Therefore, that version of the tokenizer does not contain the character `z` in its vocabulary and so would convert that character to an unknown token if the model was used to tokenize real data (actually, error handling was not added to instruct the model to create unknown tokens and so it would crash, but for productionised models this is the case).\n",
        "\n",
        "The BPE tokenizers used in GPT-2 and RoBERTa do not have this issue due to a trick within the code. Instead of analysing the training data based on the Unicode characters, they instead analyse the character's bytes. This is called **Byte-Level BPE** and allows a small base vocabulary to be able to tokenize all characters the model might see.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "XJRqnK6mMnt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 - WordPiece Tokenization\n",
        "\n",
        "**WordPiece** is a tokenization method developed by Google for their seminal BERT model, and is used in its derivative models such as DistilBERT and MobileBERT.\n",
        "\n",
        "The full details of the WordPiece algorithm have not been fully released to the public, and so the methodology presented in this notebook is based on the interpretation given by Hugging Face [12]. The WordPiece algorithm is similiar to BPE, but uses a different metric to determine the merge rules. Instead of choosing the most frequent character pair, a score is calculated for each pair instead, and the pair with the highest score determines which characters are merged. WordPiece is trained as follows:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 1) Construct the Corpus**\n",
        "\n",
        "Again, the input text is given to the normalization and pre-tokenization models to create clean words. The words are then given to the WordPiece model, which determines the frequency of each word, and stores this number alongside the word in a list called the corpus.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 2) Construct the Vocabulary**\n",
        "\n",
        "As with BPE, the words from the corpus are then broken down individual characters and are added to an empty list called the vocabulary. However this time, instead of storing simply each individual character, two # symbols are used as markers to determine whether the character was found at the beginning of a word, or from the middle/end of a word. For example, the word `cat` would be split into `['c', 'a', 't']` in BPE, but in WordPiece it would look like `['c', '##a', '##t']`. In this system, `c` at the beginning of a word, and `##c` from the middle or end of a word, would be treated differently. As before, he algorithm will iteratively add to this vocabulary every time it determines which character pairs can be merged together.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 3) Calculate the Pair Score for Each Adjacent Character Pair**\n",
        "\n",
        "Unlike the BPE model, this time a score is calculated for each of the character pairs. First, each adjacent character pair in the corpus is identified, e.g. `'c##a'`, `##a##t` and so on, and the frequency is counted. The frequency of each character individually is also determined. With these values known, a pair score can then be calculated according to the following formula:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "$\n",
        "    \\begin{align}\n",
        "    \\frac{\\text{freq of pair}}{\\text{freq of first element} \\space \\cdot{} \\space{} \\text{freq of second element}}\n",
        "    \\end{align}\n",
        "$\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "This metric assigns higher scores to characters that appear frequently together, but less frequently on their own or with other characters. This is the main difference between WordPiece and BPE, since BPE does not take into account the overall frequency of the individual characters on their own.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 4) Create a Merging Rule**\n",
        "\n",
        "High scores represent character pairs that are commonly seen together. That is, if `c##a` has a high pair score, then `c` and `a` are frequently seen together in the corpus, and not so frequently seen separately. Just as with BPE, the merging rule is determined by taking the character pair with the highest score, but this time instead of frequency determining the score, its the pair score.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 5) Repeat Steps 3 and 4**\n",
        "\n",
        "Steps 3 and are then repeated, finding more merging rules, and adding more character pairs to the vocabulary. This process continues until the vocabulary size reaches a target size specified at the beginning of the training.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "The cell below shows an implementation of WordPiece which inherits from the BPE model written earlier.\n",
        "\n",
        "&nbsp;"
      ],
      "metadata": {
        "id": "KQrO3jWrujPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordPiece(BPE):\n",
        "\n",
        "    def add_hashes(self, word):\n",
        "        ''' Add # symbols to every character in a word except the first.\n",
        "\n",
        "            Take in a word as a string and add # symbols to every character\n",
        "            except the first. Return the result as a list where each element is\n",
        "            a character with # symbols in front, except the first character\n",
        "            which is just the plain character.\n",
        "\n",
        "            Args:\n",
        "                word (str): The word to add # symbols to.\n",
        "\n",
        "            Returns:\n",
        "                hashed_word (list): A list of the characters with # symbols\n",
        "                                    (except the first character whihc is just\n",
        "                                    the plain character).\n",
        "        '''\n",
        "        hashed_word = [word[0]]\n",
        "\n",
        "        for char in word[1:]:\n",
        "            hashed_word.append(f'##{char}')\n",
        "\n",
        "        return hashed_word\n",
        "\n",
        "\n",
        "    def create_merge_rule(self, corpus):\n",
        "        ''' Create a merge rule and add it to the self.merge_rules list.\n",
        "\n",
        "            Args:\n",
        "                corpus (list[tuple(list, int)]):\n",
        "                               A list of tuples where the first element is a\n",
        "                               list of a word in the words list (where the\n",
        "                               elements are the individual characters (or\n",
        "                               subwords in later iterations) of the\n",
        "                               word), and the second element is an integer\n",
        "                               representing the frequency of the word in the\n",
        "                               list.\n",
        "\n",
        "            Returns:\n",
        "                None\n",
        "        '''\n",
        "        pair_frequencies = self.find_pair_frequencies(corpus)\n",
        "        char_frequencies = self.find_char_frequencies(corpus)\n",
        "        pair_scores = self.find_pair_scores(pair_frequencies, char_frequencies)\n",
        "\n",
        "        highest_scoring_pair = max(pair_scores, key=pair_scores.get)\n",
        "        self.merge_rules.append(highest_scoring_pair.split(','))\n",
        "        self.vocabulary.append(highest_scoring_pair)\n",
        "\n",
        "\n",
        "    def create_vocabulary(self, words):\n",
        "        ''' Create a list of every unique character in a list of words.\n",
        "\n",
        "            Unlike the BPE algorithm where each character is stored normally,\n",
        "            here a distinction is made by characters that begin a word\n",
        "            (unmarked), and characters that are in the middle or end of a word\n",
        "            (marked with a '##'). For example, the word 'cat' will be split into\n",
        "            ['c', '##a', '##t'].\n",
        "\n",
        "            Args:\n",
        "                words (list): A list of strings containing the words of the\n",
        "                                input text.\n",
        "\n",
        "            Returns:\n",
        "                vocabulary (list): A list of every unique character in the list\n",
        "                                    of input words, marked accordingly with ##\n",
        "                                    to denote if the character was featured in\n",
        "                                    the middle/end of a word, instead of as the\n",
        "                                    first character of the word.\n",
        "        '''\n",
        "        vocabulary = set()\n",
        "        for word in words:\n",
        "            vocabulary.add(word[0])\n",
        "            for char in word[1:]:\n",
        "                vocabulary.add(f'##{char}')\n",
        "\n",
        "        # Convert to list so the vocabulary can be appended to later\n",
        "        vocabulary = list(vocabulary)\n",
        "        return vocabulary\n",
        "\n",
        "\n",
        "    def find_char_frequencies(self, corpus):\n",
        "        ''' Find the frequency of each character in the corpus.\n",
        "\n",
        "            Loop through the corpus and calculate the frequency of characters.\n",
        "            Note that 'c' and '##c' are different characters, since the first\n",
        "            represents a 'c' at the start of a word, and '##c' represents a 'c'\n",
        "            in the middle/end of a word. Return a dictionary of each character\n",
        "            pair as the keys and the corresponding frequency as the values.\n",
        "\n",
        "            Args:\n",
        "                corpus (list[tuple(list, int)]):\n",
        "                               A list of tuples where the first element is a\n",
        "                               list of a word in the words list (where the\n",
        "                               elements are the individual characters (or\n",
        "                               subwords in later iterations) of the\n",
        "                               word), and the second element is an integer\n",
        "                               representing the frequency of the word in the\n",
        "                               list.\n",
        "\n",
        "            Returns:\n",
        "                pair_freq_dict (dict): A dictionary where the keys are the\n",
        "                                       characters from the input corpus and the\n",
        "                                       values are an integer representing\n",
        "                                       the frequency.\n",
        "        '''\n",
        "        char_frequencies = dict()\n",
        "\n",
        "        for word, word_freq in corpus:\n",
        "            for char in word:\n",
        "                if char in char_frequencies:\n",
        "                    char_frequencies[char] += word_freq\n",
        "                else:\n",
        "                    char_frequencies[char] = word_freq\n",
        "\n",
        "        return char_frequencies\n",
        "\n",
        "\n",
        "    def find_pair_scores(self, pair_frequencies, char_frequencies):\n",
        "        ''' Find the pair score for each character pair in the corpus.\n",
        "\n",
        "            Loops through the pair_frequencies dictionary and calculate the pair\n",
        "            score for each pair of adjacent characters in the corpus. Store the\n",
        "            scores in a dictionary and return it.\n",
        "\n",
        "            Args:\n",
        "                pair_frequencies (dict):\n",
        "                               A dictionary where the keys are the adjacent\n",
        "                               character pairs in the corpus and the values are\n",
        "                               the frequencies of each pair.\n",
        "\n",
        "                char_frequencies (dict):\n",
        "                               A dictionary where the keys are the characters in\n",
        "                               the corpus and the values are corresponding\n",
        "                               frequencies.\n",
        "\n",
        "            Returns:\n",
        "                pair_scores (dict): A dictionary where the keys are the\n",
        "                                    adjacent character pairs in the input corpus\n",
        "                                    and the values are the corresponding pair\n",
        "                                    score.\n",
        "        '''\n",
        "        pair_scores = dict()\n",
        "\n",
        "        for pair in pair_frequencies.keys():\n",
        "            char_1 = pair.split(',')[0]\n",
        "            char_2 = pair.split(',')[1]\n",
        "            denominator = (char_frequencies[char_1]*char_frequencies[char_2])\n",
        "            score = (pair_frequencies[pair]) / denominator\n",
        "            pair_scores[pair] = score\n",
        "\n",
        "        return pair_scores\n",
        "\n",
        "\n",
        "    def get_merged_chars(self, char_1, char_2):\n",
        "        ''' Merge the highest score pair and return to the self.merge method.\n",
        "\n",
        "            Remove the # symbols as necessary and merge the highest scoring pair\n",
        "            then return the merged characters to the self.merge method.\n",
        "\n",
        "\n",
        "            Args:\n",
        "                char_1 (str): The first character in the highest-scoring pair.\n",
        "                char_2 (str): The second character in the highest-scoring pair.\n",
        "\n",
        "            Returns:\n",
        "                merged_chars (str): Merged characters.\n",
        "        '''\n",
        "        if char_2.startswith('##'):\n",
        "            merged_chars = char_1 + char_2[2:]\n",
        "        else:\n",
        "            merged_chars = char_1 + char_2\n",
        "\n",
        "        return merged_chars\n",
        "\n",
        "\n",
        "    def initialise_corpus(self, words):\n",
        "        ''' Split each word into characters and count the word frequency.\n",
        "\n",
        "            Split each word in the input word list on every character. For each\n",
        "            word, store the split word in a list as the first element inside a\n",
        "            tuple. Store the frequency count of the word as an integer as the\n",
        "            second element of the tuple. Create a tuple for every word in this\n",
        "            fashion and store the tuples in a list called 'corpus', then return\n",
        "            then corpus list.\n",
        "\n",
        "            Args:\n",
        "                None.\n",
        "\n",
        "            Returns:\n",
        "                corpus (list[tuple(list, int)]):\n",
        "                               A list of tuples where the first element is a\n",
        "                               list of a word in the words list (where the\n",
        "                               elements are the individual characters of the\n",
        "                               word), and the second element is an integer\n",
        "                               representing the frequency of the word in the\n",
        "                               list.\n",
        "        '''\n",
        "        corpus = self.calculate_frequency(words)\n",
        "        corpus = [(self.add_hashes(word), freq) for (word, freq) in corpus]\n",
        "        return corpus\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        ''' Take in some text and return a list of tokens for that text.\n",
        "\n",
        "            Args:\n",
        "                text (str): The text to be tokenized.\n",
        "\n",
        "            Returns:\n",
        "                tokens (list): The list of tokens created from the input text.\n",
        "        '''\n",
        "        # Create a cleaned vocabulary list without # and commas to check against\n",
        "        clean_vocabulary = [word.replace('#', '').replace(',', '') for word in self.vocabulary]\n",
        "        clean_vocabulary.sort(key=lambda word: len(word))\n",
        "        clean_vocabulary = clean_vocabulary[::-1]\n",
        "\n",
        "        # Break down the text into the largest tokens first, then smallest\n",
        "        remaining_string = text\n",
        "        tokens = []\n",
        "        keep_checking = True\n",
        "\n",
        "        while keep_checking:\n",
        "            keep_checking = False\n",
        "            for vocab in clean_vocabulary:\n",
        "                if remaining_string.startswith(vocab):\n",
        "                    tokens.append(vocab)\n",
        "                    remaining_string = remaining_string[len(vocab):]\n",
        "                    keep_checking = True\n",
        "\n",
        "        if len(remaining_string) > 0:\n",
        "            tokens.append(remaining_string)\n",
        "\n",
        "        return tokens\n"
      ],
      "metadata": {
        "id": "nMWL53HoHrGM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.8 - Training WordPiece  with a Toy Dataset\n",
        "\n",
        "The WordPiece algorithm is trained below with the same toy dataset that was given to the BPE algorithm, and this time the tokens that it learns are very different. It is clear to see that WordPiece favours character combinations where the characters appear more commonly with each other than without, and so `m` and `p` are merged immediately since they only exist in the dataset together and not alone. The idea here is to force the model to consider what is being lost be merging characters together. That is, are these characters always together? In that case the characters should be merged as they are clearly one unit. Alternatively, are the characters very frequent in the corpus any way? In that case the characters may just be common and so appear next to other tokens by virtue of the abundance in the dataset [13].\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "bRsCZrZwTGeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wp = WordPiece()\n",
        "wp.train(words, 30)\n",
        "\n",
        "print(f'INITIAL CORPUS:\\n{wp.corpus_history[0]}\\n')\n",
        "for rule, corpus in list(zip(wp.merge_rules, wp.corpus_history[1:])):\n",
        "    print(f'NEW MERGE RULE: Combine \"{rule[0]}\" and \"{rule[1]}\"')\n",
        "    print(corpus, end='\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKiaSJGNTG8X",
        "outputId": "9bcc094d-a066-48e1-d55e-687e5885ce7f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INITIAL CORPUS:\n",
            "[(['c', '##a', '##t'], 5), (['c', '##a', '##t', '##s'], 2), (['e', '##a', '##t'], 10), (['e', '##a', '##t', '##i', '##n', '##g'], 3), (['r', '##u', '##n', '##n', '##i', '##n', '##g'], 2), (['j', '##u', '##m', '##p', '##i', '##n', '##g'], 1), (['f', '##o', '##o', '##d'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"##m\" and \"##p\"\n",
            "[(['c', '##a', '##t'], 5), (['c', '##a', '##t', '##s'], 2), (['e', '##a', '##t'], 10), (['e', '##a', '##t', '##i', '##n', '##g'], 3), (['r', '##u', '##n', '##n', '##i', '##n', '##g'], 2), (['j', '##u', '##mp', '##i', '##n', '##g'], 1), (['f', '##o', '##o', '##d'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"r\" and \"##u\"\n",
            "[(['c', '##a', '##t'], 5), (['c', '##a', '##t', '##s'], 2), (['e', '##a', '##t'], 10), (['e', '##a', '##t', '##i', '##n', '##g'], 3), (['ru', '##n', '##n', '##i', '##n', '##g'], 2), (['j', '##u', '##mp', '##i', '##n', '##g'], 1), (['f', '##o', '##o', '##d'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"j\" and \"##u\"\n",
            "[(['c', '##a', '##t'], 5), (['c', '##a', '##t', '##s'], 2), (['e', '##a', '##t'], 10), (['e', '##a', '##t', '##i', '##n', '##g'], 3), (['ru', '##n', '##n', '##i', '##n', '##g'], 2), (['ju', '##mp', '##i', '##n', '##g'], 1), (['f', '##o', '##o', '##d'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"ju\" and \"##mp\"\n",
            "[(['c', '##a', '##t'], 5), (['c', '##a', '##t', '##s'], 2), (['e', '##a', '##t'], 10), (['e', '##a', '##t', '##i', '##n', '##g'], 3), (['ru', '##n', '##n', '##i', '##n', '##g'], 2), (['jump', '##i', '##n', '##g'], 1), (['f', '##o', '##o', '##d'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"jump\" and \"##i\"\n",
            "[(['c', '##a', '##t'], 5), (['c', '##a', '##t', '##s'], 2), (['e', '##a', '##t'], 10), (['e', '##a', '##t', '##i', '##n', '##g'], 3), (['ru', '##n', '##n', '##i', '##n', '##g'], 2), (['jumpi', '##n', '##g'], 1), (['f', '##o', '##o', '##d'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"##i\" and \"##n\"\n",
            "[(['c', '##a', '##t'], 5), (['c', '##a', '##t', '##s'], 2), (['e', '##a', '##t'], 10), (['e', '##a', '##t', '##in', '##g'], 3), (['ru', '##n', '##n', '##in', '##g'], 2), (['jumpi', '##n', '##g'], 1), (['f', '##o', '##o', '##d'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"ru\" and \"##n\"\n",
            "[(['c', '##a', '##t'], 5), (['c', '##a', '##t', '##s'], 2), (['e', '##a', '##t'], 10), (['e', '##a', '##t', '##in', '##g'], 3), (['run', '##n', '##in', '##g'], 2), (['jumpi', '##n', '##g'], 1), (['f', '##o', '##o', '##d'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"run\" and \"##n\"\n",
            "[(['c', '##a', '##t'], 5), (['c', '##a', '##t', '##s'], 2), (['e', '##a', '##t'], 10), (['e', '##a', '##t', '##in', '##g'], 3), (['runn', '##in', '##g'], 2), (['jumpi', '##n', '##g'], 1), (['f', '##o', '##o', '##d'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"jumpi\" and \"##n\"\n",
            "[(['c', '##a', '##t'], 5), (['c', '##a', '##t', '##s'], 2), (['e', '##a', '##t'], 10), (['e', '##a', '##t', '##in', '##g'], 3), (['runn', '##in', '##g'], 2), (['jumpin', '##g'], 1), (['f', '##o', '##o', '##d'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"runn\" and \"##in\"\n",
            "[(['c', '##a', '##t'], 5), (['c', '##a', '##t', '##s'], 2), (['e', '##a', '##t'], 10), (['e', '##a', '##t', '##in', '##g'], 3), (['runnin', '##g'], 2), (['jumpin', '##g'], 1), (['f', '##o', '##o', '##d'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"##in\" and \"##g\"\n",
            "[(['c', '##a', '##t'], 5), (['c', '##a', '##t', '##s'], 2), (['e', '##a', '##t'], 10), (['e', '##a', '##t', '##ing'], 3), (['runnin', '##g'], 2), (['jumpin', '##g'], 1), (['f', '##o', '##o', '##d'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"runnin\" and \"##g\"\n",
            "[(['c', '##a', '##t'], 5), (['c', '##a', '##t', '##s'], 2), (['e', '##a', '##t'], 10), (['e', '##a', '##t', '##ing'], 3), (['running'], 2), (['jumpin', '##g'], 1), (['f', '##o', '##o', '##d'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"jumpin\" and \"##g\"\n",
            "[(['c', '##a', '##t'], 5), (['c', '##a', '##t', '##s'], 2), (['e', '##a', '##t'], 10), (['e', '##a', '##t', '##ing'], 3), (['running'], 2), (['jumping'], 1), (['f', '##o', '##o', '##d'], 6)]\n",
            "\n",
            "NEW MERGE RULE: Combine \"f\" and \"##o\"\n",
            "[(['c', '##a', '##t'], 5), (['c', '##a', '##t', '##s'], 2), (['e', '##a', '##t'], 10), (['e', '##a', '##t', '##ing'], 3), (['running'], 2), (['jumping'], 1), (['fo', '##o', '##d'], 6)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.9 - Tokenizing using WordPiece\n",
        "\n",
        "Now that the WordPiece algorithm has been trained (i.e. now that all have the merging rules have been found), the model can be used to tokenize any text by first splitting each of the words on every character, and then finding the largest token it can at the start of the string, then finding the largest token it can for the remaining part of the string. This process repeats until no more of the strings matches a known token from the training data and so the remaining part of the string is take as the final token.\n",
        "\n",
        "Given the very limited training data, the tokens that the model has been able to learn are quite strange, but interesting nonetheless. With the model trained on this training data, we can see the following performance on the nonsense string `runntjumpy`. First the string is broken into `['runn','tjumpy']`, since `runn` is the largest token from the training set that can be found at the start of the word. Next, the largest token that can be found at the start of `tjumpy` given the training data is just the letter `t`, and so now the tokens are `['runn', 't', 'jumpy']`. After this, `jump` can be found and so the tokens become `['runn', 't', 'jump', 'y']`. Finally, the letter `y` does not appear in the training data, so is added as the last token.\n",
        "\n",
        "&nbsp;"
      ],
      "metadata": {
        "id": "MgPfp6Rjik5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wp.tokenize('runntjumpy'))\n",
        "print(wp.tokenize('runningandjumping'))\n",
        "print(wp.tokenize('jumper'))\n",
        "print(wp.tokenize('unknown'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RstaNj4-aq05",
        "outputId": "0cdb4fd1-af4a-4c22-e506-cbceb8e8a034"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['runn', 't', 'j', 'u', 'm', 'p', 'y']\n",
            "['running', 'a', 'n', 'd', 'jumping']\n",
            "['jump', 'e', 'r']\n",
            "['u', 'n', 'known']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.10 - Unigram Tokenization\n",
        "\n",
        "\n",
        "Unigram tokenizers take a different approach to BPE and WordPiece, by starting with a large vocabulary, and iteratively decreasing it until the desired size is reached, instead of the other way around.\n",
        "\n",
        "The Unigram model uses a statistical approach, where the probability of each word or character in a sentence is considered. For example in the sentence `This is an example sentence`, it can be split into either `['This', 'is', 'an', 'example', 'sentence']` or `['T', 'h' , 'i', 's', '_i', 's', '_a', 'n','_e', 'x', 'a', 'm', 'p', 'l', 'e', '_s', 'e', 'n', 't', 'e', 'n', 'c', 'e']`. Notice how in the case where the sentence is split by characters, an underscore is prepended to the beginning of each character where a space would have been used to mark the start of a new word in the original sentence.\n",
        "\n",
        "Each element in these lists can be considered a token, $t$, and the probability of the series of tokens, $t_1, t_2, ..., t_n$, occuring being given by:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "$\n",
        "    P(t_1, t_2, ..., t_n) = P(t_1) \\space \\cdot P(t_2) \\space \\cdot \\space ... \\space \\cdot \\space P(t_n)\n",
        "$\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "The Unigram model is trained using the following steps:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 1) Construct the Corpus**\n",
        "\n",
        "As always, the input text is given to the normalization and pre-tokenization models to create clean words. The words are then given to the Unigram model, which determines the frequency of each word, and stores this number alongside the word in a list called the corpus.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 2) Construct the Vocabulary**\n",
        "\n",
        "The vocabulary size of the Unigram model starts out very large, and is iteratively decreased until the desired size is reached. To construct the initial vocabulary, find every possible substring in the corpus. For example, if the first word in the corpus is 'cats', the substrings `['c', 'a', 't', 's', 'ca', 'at', 'ts', 'cat', 'ats']` will be added to the vocabulary.\n",
        "\n",
        "**Step 3) Calculate the Probability of Each Token**\n",
        "\n",
        "The probability of a token is approximated by finding the number of occurrences of the token in the corpus, and dividing by the total number of token occurrences.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "$\n",
        "    P(t) = \\frac{\\text{frequency of t}}{\\text{frequency of all tokens}}\n",
        "$\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 4) Find All the Possible Segmentations of the Word**\n",
        "\n",
        "Consider the case where a word from the training corpus is `cat`. This can be segemented in the following ways:\n",
        "\n",
        "`['c', 'a', 't']`\n",
        "\n",
        "`['ca', 't']`\n",
        "\n",
        "`['c', 'at']`\n",
        "\n",
        "`['cat']`\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 5) Calculate the Approximate Probability of Each Segmentation Occuring in the Corpus**\n",
        "\n",
        "Using the rule $P(t_1, t_2, ..., t_n) = \\frac{\\text{frequency of t}}{\\text{frequency of all tokens}}$, the probability for each series of tokens can be calculated. As an example, this might look like:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "$P(c, a, t) = P(c) \\space \\cdot P(a) \\space \\cdot P(t) = \\frac{30}{100} \\cdot \\frac{20}{100} \\cdot \\frac{25}{100} = 0.015$\n",
        "\n",
        "$P(ca, t) = P(ca) \\space \\cdot P(t) = \\frac{5}{100} \\cdot \\frac{20}{100} = 0.01$\n",
        "\n",
        "$P(c, at) = P(c) \\space \\cdot P(at) = \\frac{30}{100} \\cdot \\frac{15}{100} = 0.045$\n",
        "\n",
        "$P(cat) = \\frac{5}{100} = 0.05 $\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Since the segmentation `['ca', 't']` has the highest probability score, this is the segmentation that is used to tokenize the word. So the word `cat` will be tokenized as `['ca', 't']`. You can imagine that for longer words such as `tokenization`, splits may occur in multiple places throughout the word, for example `['token', 'iza', tion]` or `['token', 'ization]`.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Step 6) Calculate the Loss**\n",
        "\n",
        "The term **loss** refers to a score for the model, such that if an important token is removed from the vocabulary the loss increases a lot, but if a less important token is removed the loss increases a lot. By calculating what the loss would be in the model for each token if it is was removed, it is possible to find the token that is the least useful in the vocabulary. This can be repeated iteratively until the vocabulary size has decreased to only the most useful tokens remain from the training set (corpus). The loss is given by:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "$\n",
        "\\sum{(\\text{frequency of word}) \\cdot (-log(P(\\text{word})))}\n",
        "$\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Once the enough characters have been removed to bring the vocabulary down to the desired size, the training is finished and the model can be used to tokenize words.\n"
      ],
      "metadata": {
        "id": "oG6DHesDtmdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.12 - Comparing BPE, WordPiece, and Unigram\n",
        "\n",
        "Depending on the training set and the data to be tokenized, some tokenizers may perform better than others. When choosing a tokenizer for a language model, it may be best to experiment with the training set used for a specific use-case and see which gives the best results. However, there are some general tendancies of these three tokenizers that are useful to discuss.\n",
        "\n",
        "Of the three, BPE seems to be the most popular choice for current language model tokenizers. Although in a such a rapidly changing space, this is likely to change in the future. In fact, other subword tokenizers such as SetencePiece are gaining much more popularity in recent times [14].\n",
        "\n",
        "WordPiece seems to produce more single-word tokens compared to BPE and Unigram, but irrespective of model choice all tokenizers seem to produce fewer tokens as the vocabulary sizes increases [15].\n",
        "\n",
        "Ultimately however, the choice of tokenizer depends on the datasets that are intended to be used with the model. Although a safe bet may be to try BPE or SentencePiece, and experiment from there.\n",
        "\n",
        "&nbsp;"
      ],
      "metadata": {
        "id": "m5mbpKTNVvjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.13 - Post-Processing\n",
        "\n",
        "The final step of the tokenization pipeline is **post-processing**, and is where any final modifications can be made to the output if necessary. BERT famously uses this step to add two additional types of tokens:\n",
        "\n",
        "* `[CLS]` - this token stands for 'classification', and is used to mark beginning of the input text. This is required since one of the training tasks for BERT is sentence classification (hence the name of the token).\n",
        "\n",
        "* `[SEP]` - this token stands for 'separation'  and is used to separate sentences in the input. This is useful for many tasks that BERT performs, including handling multiple instructions at once in the same prompt [16].\n",
        "\n",
        "&nbsp;"
      ],
      "metadata": {
        "id": "WxSUD9mnxAwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align=\"center\">Section 4 - Tokenizers in Python Libraries</h2>"
      ],
      "metadata": {
        "id": "zhj1eD6ktUme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 - Overview of the Hugging Face `tokenizers` Library\n",
        "\n",
        "Hugging Face provides the`tokenizers` library which is available in several programming languages, including bindings for Python. The library contains a generic `Tokenizer` class which allows the user to work with pre-trained models, the full list of which can be found on the Hugging Face website [17]. In addition, the library also contains four pre-made but untrained models that the user can train with their own data [18]. These are useful for creating specific tokenizers that are tuned to a particular type of document. The cells below show exmaples of working with both the pre-trained and untrained tokenizers in Python.\n",
        "\n",
        "&nbsp;"
      ],
      "metadata": {
        "id": "JiJ9uiFZYDOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 - Using Pre-Trained Tokenizers\n",
        "\n",
        "The `tokenizers` library makes it very easy to use a pre-trained tokenizer. Simply import the `Tokenizer` class, and called the `from_pretrained` method and pass in the name of the model to use the tokenizer from. A list of models is given in [17].\n",
        "\n",
        "&nbsp;"
      ],
      "metadata": {
        "id": "ujohnW3xjhiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "id": "HC1zELrflbZr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 - Training Pre-made Tokenizers\n",
        "\n",
        "To use one of the pre-made but untrained tokenizers, simply import the desired model from the `tokenizers` library, and create an instance of the model class. As stated above, there for four models included in the library:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "* `BertWordPieceTokenizer` - The famous Bert tokenizer, using WordPiece\n",
        "\n",
        "* `CharBPETokenizer` - The original BPE\n",
        "\n",
        "* `ByteLevelBPETokenizer` - The byte level version of the BPE\n",
        "\n",
        "* `SentencePieceBPETokenizer` - A BPE implementation compatible with the one used by SentencePiece\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "To train the model, use the `train` method and pass the path to a file (or list of paths to files) containing the training data. Once trained, the model can then be used to tokenize some text using the `encode` method. Finally, the trained tokenizer can be saved using the `save` method so that training does not have to be performed again. The cell below shows some example code which has been adapted from the examples available on the Hugging Face Tokenizers GitHub page [17].\n",
        "\n",
        "&nbsp;"
      ],
      "metadata": {
        "id": "msL2JqD2d8Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import a tokenizer\n",
        "from tokenizers import BertWordPieceTokenizer, CharBPETokenizer, ByteLevelBPETokenizer, SentencePieceBPETokenizer\n",
        "\n",
        "# Instantiate the model\n",
        "tokenizer = CharBPETokenizer()\n",
        "\n",
        "# Train the model\n",
        "tokenizer.train(['./path/to/files/1.txt', './path/to/files/2.txt'])\n",
        "\n",
        "# Tokenize some text\n",
        "encoded = tokenizer.encode('I can feel the magic, can you?')\n",
        "\n",
        "# Save the model\n",
        "tokenizer.save('./path/to/directory/my-bpe.tokenizer.json')"
      ],
      "metadata": {
        "id": "H1tmetppQZwh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 - Building Custom Tokenizers\n",
        "\n",
        "The `tokenizer` library also provides the components to construct a custom tokenizer very quickly, without having to implement an entire model from scratch as shown in the previous section. The cell below shows an example taken from the Hugging Face GitHub page [18] which shows how to customise the pre-tokenization and decoding stage of the tokenizer. In this case, prefix spaces were added in the pre-tokenization stage, and the decoder chosen was the ByteLevel decoder. A full list of customisation options is available in the Hugging Face documentation [19].\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "hGwC2--hfFm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "\n",
        "# Customize pre-tokenization and decoding\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
        "tokenizer.decoder = decoders.ByteLevel()\n",
        "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
        "\n",
        "# And then train\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=20000,\n",
        "    min_frequency=2,\n",
        "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
        ")\n",
        "tokenizer.train([\n",
        "    \"./path/to/dataset/1.txt\",\n",
        "    \"./path/to/dataset/2.txt\",\n",
        "    \"./path/to/dataset/3.txt\"\n",
        "], trainer=trainer)\n",
        "\n",
        "# And Save it\n",
        "tokenizer.save(\"byte-level-bpe.tokenizer.json\", pretty=True)"
      ],
      "metadata": {
        "id": "xLVdw3LFpFav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align=\"center\">Section 5 - Conclusion</h2>\n",
        "\n",
        "The tokenization pipeline is a crucial part of any language model, and careful consideration should be given when deciding which type of tokenizer to use. Nowadays, many of these decisions have been made for us by the developers of libraries such as those provided by Hugging Face. These allow users to quickly train and used language models with custom data. However, a solid understanding of tokenization methods is invaluable for fine-tuning models, and squeezing out additional performance on different datasets.\n",
        "\n",
        "&nbsp;\n"
      ],
      "metadata": {
        "id": "oeecNXLqqvbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align=\"center\">Section 6 - Glossary</h2>\n",
        "\n",
        "**Byte-Level BPE**\n",
        "> A trick used by the GPT-2 and RoBERTa models (among others) whereby characters in the training data for a BPE algorithm are analysis using their bytes instead of the Unicode representation. This eliminates out of vocabulary words since every word can be tokenized.\n",
        "\n",
        "**Byte Pair Encoding (BPE)**\n",
        "> A subword tokenization algorithm that is a commonly-used in many transformer models such as Open AI's GPT and GPT-2 models, BART, and many others. The BPE algorithm aims to decompose a string of text into subword units that appear frequently in a reference corpus (the text used to train the tokenization model)\n",
        "\n",
        "**Character-based tokenization**\n",
        "> A tokenization method in which the a sentence/sentences are split on each character, including letters, numbers, and special characters such as punctuation.\n",
        "\n",
        "**Encoding**\n",
        "> The process of replacing the textual tokens with a number representation.\n",
        "\n",
        "**Exploding Vocabulary Problem**\n",
        "> An issue with word-based tokenization methods in which the number of words in a models vocabulary increases to very high levels (e.g. greater than the number of words in the English language).\n",
        "\n",
        "**Decoding**\n",
        "> The process of converting encoded tokens back into text.\n",
        "\n",
        "**Normalization**\n",
        "> The process of cleaning text before it is split into tokens. This includes converting each character to lowercase, removing accents from characters (e.g. 'é' becomes 'e'), removing unnecessary whitespace, and so on.\n",
        "\n",
        "**Out-of-Vocabulary (OOV)**\n",
        "> Words which are not found in the vocabulary of a model.\n",
        "\n",
        "**Post-Processing**\n",
        "> The final step of the tokenization pipeline, and is where any final modifications can be made to the output if necessary.\n",
        "\n",
        "**Pre-tokenization**\n",
        "> The first splitting of the raw text in the tokenization pipeline. The split is performed to give an upper bound to what the final tokens could be at the end of the pipeline. That is, a sentence can be split into words in the pre-tokenization step, then in the model step some of these words may be split further according to the tokenization method (e.g. subword-based).\n",
        "\n",
        "**Subword-based tokenization**\n",
        "> A tokenization methods which aims to achieve the benefits of both word-based and character-based methods, by splitting sentences within words.\n",
        "\n",
        "**Token**\n",
        "> An instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing.\n",
        "\n",
        "**Tokenization pipeline**\n",
        "> The series of actions that are taken to convert raw text into tokens. The steps of this pipeline are: normalization, pre-tokenization, model post-processing.\n",
        "\n",
        "**Tokenizer**\n",
        "> A model which converts strings of text into tokens. The tokens can then be associated with a number, which allows human lanuage to be expressed in a form computers can understand.\n",
        "\n",
        "**Unigram**\n",
        "> A subword tokenization method that starts with a large vocabulary, which iteratively decreases in size until the desired size is reached, unlike BPE and WordPiece which iteratively increase their vocavulary size.\n",
        "\n",
        "**Vocabulary**\n",
        "> The complete set of possible tokens in a tokenization model.\n",
        "\n",
        "**Word-Based Tokenization**\n",
        "> A tokenization method in which a sentence/sentences is split into words by splitting on each space in the sentence (sometimes called 'whitespace-based tokenization), or by a similar set of rules (such as punctuation-based tokenization, treebank tokenization, etc).\n",
        "\n",
        "**WordPiece**\n",
        "> A subword tokenization method developed by Google for their seminal BERT model, and was used in derivative models such as DistilBERT and MobileBERT.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RhhfLJ6QqyVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align=\"center\">Section 7 - Further Reading</h2>\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "[1] - Token Definition [Stanford NLP Group](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html#:~:text=A%20token%20is%20an%20instance,useful%20semantic%20unit%20for%20processing.)\n",
        "\n",
        "[2] Word Tokenizers - [Towards Data Science](https://towardsdatascience.com/top-5-word-tokenizers-that-every-nlp-data-scientist-should-know-45cc31f8e8b9#:~:text=Tokenization%20is%20the%20process%20of,I%E2%80%9D%20and%20%E2%80%9Cwon%E2%80%9D.)\n",
        "\n",
        "[3] Tokenizers - [Hugging Face](https://huggingface.co/docs/transformers/tokenizer_summary)\n",
        "\n",
        "[4] TransformerXL Paper - [ArXiv](https://arxiv.org/abs/1901.02860)\n",
        "\n",
        "[5] Word-Based, Subword, and Character-Based Tokenizers - [Towards Data Science](https://towardsdatascience.com/word-subword-and-character-based-tokenization-know-the-difference-ea0976b64e17)\n",
        "\n",
        "[6] A Comprehensive Guide to Subword Tokenizers - [Towards Data Science](https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c)\n",
        "\n",
        "[7] The Tokenization Pipeline - [Hugging Face](https://huggingface.co/docs/tokenizers/pipeline)\n",
        "\n",
        "[8] Pre-tokenizers - [Hugging Face](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)\n",
        "\n",
        "[9] Language Models are Unsupervised Multitask Learners - [OpenAI](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)\n",
        "\n",
        "[10] BART Model for Text Autocompletion in NLP - [Geeks for Geeks](https://www.geeksforgeeks.org/bart-model-for-text-auto-completion-in-nlp/)\n",
        "\n",
        "[11] Byte Pair Encoding - [Hugging Face](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt#byte-pair-encoding-tokenization)\n",
        "\n",
        "[12] WordPiece Tokenization - [Hugging Face](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)\n",
        "\n",
        "[13] Two minutes NLP — A Taxonomy of Tokenization Methods - [Medium](https://medium.com/nlplanet/two-minutes-nlp-a-taxonomy-of-tokenization-methods-60e330aacad3#:~:text=In%20contrast%20to%20BPE%2C%20WordPiece,to%20ensure%20it's%20worth%20it.)\n",
        "\n",
        "[14] Subword Tokenizer Comparison - [Vinija AI](https://vinija.ai/nlp/tokenizer/)\n",
        "\n",
        "[15] [A Comprehensive Analysis of Subword Tokenizers for\n",
        "Morphologically Rich Languages - [Marmara University](https://www.cmpe.boun.edu.tr/~gungort/theses/A%20Comprehensive%20Analysis%20of%20Subword%20Tokenizers%20for%20Morphologically%20Rich%20Languages.pdf)\n",
        "\n",
        "[16] How BERT Leverage Attention Mechanism and Transformer to Learn Word Contextual Relations - [Medium](https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb#:~:text=BERT%20use%20three%20embeddings%20to,separate%20segment%20(or%20sentence).)\n",
        "\n",
        "[17] List of Pretrained Models - [Hugging Face](https://huggingface.co/transformers/v3.3.1/pretrained_models.html)\n",
        "\n",
        "[18] Hugging Face Tokenizers Library - [GitHub](https://github.com/huggingface/tokenizers/blob/main/bindings/python/README.md)\n",
        "\n",
        "[19] Pre-Tokenization Documentation - [Hugging Face](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)"
      ],
      "metadata": {
        "id": "eFHhi50l9St6"
      }
    }
  ]
}